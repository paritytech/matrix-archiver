# room: #jam-conformance:matrix.org
# exported: 2026-02-09 04:04 UTC
2025-08-20 15:56 erin: tomusdrw: i've set up the archiver to archive this also, so hopefully will be up starting tomorrow
2025-08-21 06:33 ascriv: https://github.com/davxy/jam-conformance/issues/26 should we not discourage open source implementations (for now)? [edited]
2025-08-21 10:27 oliver.tale-yazdi: I think open source repos are fine. Other implementors are just not supposed to look at it
2025-08-21 10:28 oliver.tale-yazdi: Maybe in their self interest it should be private though since someone else may steal their code and submit it first, but that is up to them to decide IMO [edited]
2025-08-21 10:30 sourabhniyogi: I've never understood when we are required to make everything open-source -- is it after M1 (like, this year =) ), or would it be at the very end?  Or is there some "share with the w3feval team" process?

As the [reports](https://github.com/davxy/jam-conformance/tree/main/fuzz-reports) are clear for > 2/3 of active teams now (omg ... yay!), can we move onto 0.7.0 traces for the rest of the month and start the 0.7.0 fuzzing in first half of September, and then do 0.7.1 traces/fuzzing in the second half? [edited]
2025-08-21 10:32 oliver.tale-yazdi: I think you only need to share the code with the Milestone judges eventually. 
2025-08-21 10:45 sourabhniyogi: Does anyone want to see refining added to the fuzz protocol?
2025-08-21 10:47 ascriv: Sure. But doesnâ€™t that require networking for the erasure coding stuff? 
2025-08-21 10:56 sourabhniyogi: For a auditable work package bundle, it doesn't have to.  We can add this to the fuzzer protocol:

```
--> Work Package Bundle ++ Core Index ++ Segment Root Mappings ++ Timeslot
<-- Work Report
```
2025-08-21 10:57 jaymansfield: > <@sourabhniyogi:matrix.org> Does anyone want to see refining added to the fuzz protocol?

That would be great 
2025-08-21 11:01 sourabhniyogi: Anyone see the need to add more to the above than 1+2, or have a desire to add this in 0.7.0 vs 0.7.1?  There is this ancestors detail I'm wondering what will happen when
2025-08-21 11:02 ascriv: Iâ€™m ok with whatever the evaluators decide, but clarity/a roadmap would be nice 
2025-08-21 11:18 clearloop: I hope polkajam can join the discuss which can make our time spent more efficient, for example, the testing data format
2025-08-21 11:26 sourabhniyogi: I think the expectation is that we all become evaluators, and we earn our fellowship stripes by suggesting the changes we want ... like, how do you want to see ancestors in the data
2025-08-21 11:26 davxy: Your time schedule looks realistic and reasonable
2025-08-21 11:36 davxy: 
This seems doable. Maybe you could open a PR to extend the fuzzer protocol?

Right now, I am focused on fixing issues in the fuzzer and getting stuff ready for version 0.7.0 (test vectors first).

The refinement extension could realistically land in 0.7.1 instead.  
For 0.7.0, the fuzzer protocol should already be extended with:  
- supported-features handshake during PeerInfo message exchange  
- ancestors set  

If the target supports refinement, that capability could be included as part of the features exchange.



2025-08-21 11:37 sourabhniyogi: Is refining successfully part of M1 conformance?  M2?  
2025-08-21 11:38 sourabhniyogi: I am happy to do a refining extension PR targeting 0.7.1 and try it out with everyone here in Sept =)
2025-08-21 11:40 sourabhniyogi: I'll primarily just extend this README.md here https://github.com/davxy/jam-conformance/blob/main/fuzz-proto/README.md unless you suggest something else?
2025-08-21 11:44 davxy: From a technical standpoint, M1 is doing block importing, so as far as I can tell, the answer is no.
2025-08-21 11:54 sourabhniyogi: Can we consider the 0.6.7 fuzzer "done" (which allows other teams to roll in) or are you going to go through another few waves of 0.6.7 for the next 7-10 days, like the gas accounting wave?
2025-08-21 11:57 sourabhniyogi: For others doing fuzzers, what else should be addressed?  I think you had a task list to share maybe
2025-08-21 12:09 jaymansfield: Seems like this should work. I might attempt to build my own refine fuzzer actually as I'm getting close to finishing my recompiler. Will use the same refine arguments when I get to it next.
2025-08-21 16:03 r2rtnl: JAM Prize rules #14 and #15 appear to favor using a public GitHub repo as evidence of organic development.
Rule #7 seems reasonably interpreted as: â€œdonâ€™t look; and if you did look, disclose it publicly.â€
2025-08-21 16:16 oliver.tale-yazdi: Rule #16 also explicitly allows private repos. So not sure if either is favoured
2025-08-21 16:20 r2rtnl: Agreed â€” Iâ€™ve just shared an explanation of why TurboJam chose to go the open-source route.
2025-08-21 16:20 ascriv: Iâ€™m pretty sure private meant off-GitHub according to gavâ€™s comments last October 
2025-08-21 16:21 ascriv: image_D9A1B8B1-C051-4866-9047-C48BBDBB137B_1755793257.png
2025-08-21 16:21 ascriv: But Iâ€™m in the process of confirming this with the fellowship, can share once I hear 
2025-08-21 16:23 ascriv: â€œIf you donâ€™t want to use GitHub (private or public), add commits timestamped on a blockchainâ€
2025-08-21 16:23 oliver.tale-yazdi: But also depends on how one uses it. Just pushing commits to public GibHub wont help, it will need to be Merge Requests since they carry timestamps.
2025-08-21 16:26 oliver.tale-yazdi: Yea I am doing this to avoid any controversy later on: https://github.com/JamBrains/remark-commit (i already posted this recently but nobody seems to want to use it ðŸ˜†)
2025-08-21 16:27 ascriv: Itâ€™s definitely best to do something like OTS or your remark tool regardless. They should have required everyone do this no matter if on GitHub or off imo  [edited]
2025-08-21 16:28 ascriv: Bc as itâ€™s written and as gav explained, it seems just being on github suffices
2025-08-21 17:41 erin: archive is up: https://paritytech.github.io/matrix-archiver/archive/_21ksYpYHcVftKsUAsdMa_3Amatrix.org/index.html

cc tomusdrw 
2025-08-21 17:45 davxy: Two new highly controversial reports: 1755796851 1755796995
2025-08-21 17:46 davxy: I guess after these two we can start with 0.7.0 :-)
2025-08-21 17:46 davxy: https://github.com/davxy/jam-conformance/tree/main/fuzz-reports
2025-08-21 17:49 davxy: (the traces passed by all teams were removed from the table)
2025-08-22 14:10 danicuki: I am trying to match the preimage error because of this: https://matrix.to/#/!ddsEwXlCWnreEGuqXZ:polkadot.io/$ps8N0jq66pBJG1o26Z5XUGyU23QETifiYuVaaiiypIc?via=polkadot.io&via=matrix.org&via=parity.io

It also affects test vector https://github.com/davxy/jam-test-vectors/blob/master/stf/preimages/tiny/preimage_not_needed-1.json

2025-08-22 16:59 rustybot: > <@danicuki:matrix.org> I am trying to match the preimage error because of this: https://matrix.to/#/!ddsEwXlCWnreEGuqXZ:polkadot.io/$ps8N0jq66pBJG1o26Z5XUGyU23QETifiYuVaaiiypIc?via=polkadot.io&via=matrix.org&via=parity.io
> 
> It also affects test vector https://github.com/davxy/jam-test-vectors/blob/master/stf/preimages/tiny/preimage_not_needed-1.json
> 

Looks like you've got your answer in the other channel?
2025-08-22 17:44 danicuki: If that answer is correct than I still can't understand why some of the fuzzer cases shouldn't raise preimage_not_needed error [edited]
2025-08-22 17:46 danicuki: Cases 1755530896, 1755530728,1755531265, 1755620371
2025-08-22 17:51 ascriv: Can you provide a deeper analysis/argument for why one of these cases should error? Itâ€™s always possible we all have the same mind virus 
2025-08-22 17:56 danicuki: In all these cases

d[s]_l[(h,l)] = null 

Which according to 12.38, should fail the block. 
2025-08-22 17:58 ascriv: Iâ€™ll take a look but I think it could be because 12.38 is a definition of R, not a condition that must be true 
2025-08-22 17:58 ascriv: If that doesnâ€™t explain it then Iâ€™ll look deeper later 
2025-08-22 17:59 danicuki: All E_P must satisfy R
2025-08-22 18:01 ascriv: Right, let me look now then
2025-08-22 18:05 ascriv: I checked 1755530896, my service 2494444674 indeed does not have the preimage in the extrinsic but it does have a [] in the historical lookup  
2025-08-22 18:05 ascriv: The key for the historical lookup begins with 0x92115b..
2025-08-22 18:06 ascriv: And I see it in the pre state for â€¦.008.json for this test vector 
2025-08-22 18:06 ascriv: Value 0x00 which corresponds with empty list
2025-08-22 18:24 danicuki: I will double check on my side but I think I know what we are doing wrong. 
2025-08-23 11:22 davxy: **0.7.0 test vectors are up:**  
https://github.com/davxy/jam-test-vectors/pull/90

2025-08-23 17:54 jaymansfield: Another set of 0.7.0 safrole vectors if anyone is interested: https://github.com/javajamio/javajam-trace
2025-08-24 07:31 davxy: âš ï¸ https://github.com/davxy/jam-test-vectors/pull/90#issuecomment-3217905803
2025-08-24 07:33 davxy: For tiny we're using G_T = 20_000_000 and not 3_500_000_000 . That is why we call into accumulate twice in preimages/00000008.bin
As I described in the comment, I think a different value for tiny is reasonable to trigger interesting cases
2025-08-24 07:33 davxy: if there are no objections I'll open a PR in jamcha.in and set this as the value for G_T in tiny config
2025-08-24 08:16 davxy: https://github.com/JamBrains/jam-docs/pull/59
2025-08-24 11:13 prematurata: hey @davxy I just found out about the sbrk note in the traces readme https://github.com/davxy/jam-test-vectors/tree/v0.7.0/traces

besides the sbrk implementation the RAM code looks a bit off to me.

For example in A.42 the Writeable section starts at 2Zz + Z(|o|) while in the code there it seems to end there
2025-08-24 11:18 davxy: Was written by [@sourabhniyogi:matrix.org](https://matrix.to/#/@sourabhniyogi:matrix.org)some time ago Feel free to open a PR
2025-08-24 11:18 prematurata: ah ok maybe at that time the gp was different :)
2025-08-24 11:20 davxy: Honestly I didn't went through that go code LOL
2025-08-24 11:20 prematurata: npnp thanks for clarifying
2025-08-24 11:27 davxy: I donâ€™t feel like Iâ€™ve clarified anything :-) Iâ€™ll check that section out
2025-08-24 11:28 prematurata: well it's clear that there MAY be an error :)
2025-08-24 17:05 sourabhniyogi: Yes, it deserves an update -- the 0.6.7 fuzz traces did a thorough job of showing how its out of date (64k, read vs write OOB) -- I'm not aware of really great SBRK / heap testing expansion anywhere, and the "s" component [stack] remains elusive and entirely untouched so far as I can tell in addition to the "h" [heap] of SBRK which has not been fuzzed much.  

It would be ideal if the next 0.7.2+ fuzzing series attacked this fully and somehow incorporated "picoalloc" which I gather is the replacement to SBRK (the "h").  I am wondering if its useful to really go crazy with fuzz testing of SBRK because its going to be replaced.  We can keep busy with outeraccumulate in 0.7.0 and transfer in 0.7.1 the meantime =).   [edited]
2025-08-25 07:52 dakkk: Where can I find the source code of the service used for the tests? My implementation is raising a panic from services/bootstrap-service/src/lib.rs in two traces
  â†³ 2025-08-25 10:13 oliver.tale-yazdi: I think you can get it with `cargo clone jam-bootstrap-service`, at least that code matched with the logs that we saw when running the test vectors
  â†³ 2025-08-25 11:17 dakkk: thank you
2025-08-25 14:57 emielsebastiaan: davxy: Potential ASN error: https://github.com/davxy/jam-test-vectors/issues/93
Please review.
2025-08-26 06:50 davxy: https://github.com/davxy/jam-test-vectors/pull/94
2025-08-26 07:02 emielsebastiaan: In the office in an hour. Let me check then. It is simply the reordering. New order: i, x, z, e for both CoreActivityRecord and ServiceActivityRecord. [edited]
2025-08-26 08:24 davxy: @room Could you please check these vectors https://github.com/davxy/jam-test-vectors/pull/94 before I proceed with merging?
STF and traces. Ty

  â†³ 2025-08-26 08:37 0xjunha: fastroll passes all stf/traces
  â†³ 2025-08-26 12:10 jaymansfield: Looks good.
2025-08-26 08:32 dakkk: it works for me, and now I pass all the tests either
2025-08-26 08:45 arjanz: Traces and STF are passing
2025-08-26 08:53 vinsystems: vinwolf passes all stf and traces as well
2025-08-26 09:36 ascriv: Jamzilla passes as well
2025-08-26 10:59 danicuki: Do we already have fuzzer for 0.7.0? 
2025-08-26 11:03 danicuki: Jamixir passes all 0.7.0 traces and test vectors. 
2025-08-26 11:06 danicuki: We are still struggling with two fuzzer cases on 0.6.7 - Could anyone please provide the PVM traces for 1755531265 and 1755796995 fuzz cases (0.6.7)? 
  â†³ 2025-08-26 16:20 clearloop: 1755531265.log
  â†³ 2025-08-26 16:20 clearloop: 1755796995.log
  â†³ 2025-08-26 16:21 clearloop: hope these help â¬†ï¸ [edited]
  â†³ 2025-08-26 23:49 danicuki: â¤ï¸
  â†³ 2025-08-27 19:23 danicuki: clearloop | SpaceJam: I have a doubt about your logs:

```
2025-08-27 00:19:42 TRACE stf:accumulate: pos=26517  Ecalli               gas=9997503 regs=[804, 4278056528, 17, 16, 4278056271, 206752, 70, 2, 66858, 4, 206752, 57, 8]
2025-08-27 00:19:42 DEBUG stf:accumulate: calling host call 100
2025-08-27 00:19:42  INFO program: Bootstrap Service Accumulate, 0h @8 $18446744073709003931 target="boot"
2025-08-27 00:19:42 TRACE stf:accumulate: pos=26519  Fallthrough          gas=9997502 regs=[804, 4278056528, 17, 16, 4278056271, 206752, 70, 0, 66858, 4, 206752, 57, 8]
```
the host call is a log, so it should not change r7 from 2 to 0
the next call is a fallthrough, which should also not change r7 from 2 to 0

So why is your r7 changed to 0?  

  â†³ 2025-08-27 23:25 clearloop: good catch! we have a wrap of host calls, 0 means the host call executed successfully ðŸ¤¦â€â™‚ï¸ this could be confusing but it should not be the root case since `r7` is commonly used as exit status, and it is actually ignored by the log host call in the program, (e.g. it will be override by other operations without getting loaded )

while you can pass the traces in the test vectors, I believe the bug in the fuzz tests are in `host call` / `memory op` / `encoding stuffs`
  â†³ 2025-08-28 10:08 danicuki: I am double checking fetch.
What value do you have for G_A, G_T and G_R?
  â†³ 2025-08-28 10:11 danicuki: also, from what I know, it uses 0.7.0 spec for fetch, right?
  â†³ 2025-08-28 15:01 clearloop: the traces were generated by a commit that we haven't upgraded to 0.7.0 yet, if I'm not mistaken the fetch call updates are covered by both 0.6.7 and 0.7.0 (registers changed), from we I can recall: 

- `1755796995` is related to the threshold of accounts (if it was the last row of the table), you need to check your `info` implementation instead of `fetch`, the diff are actually in memory that register values are not that helpful
- `1755531265` I'm not sure about this, if you can share which key you failed to match, mb I can recall sth helpful
2025-08-26 11:25 davxy: Polkajam has it; I'll provide some reports in the next days
2025-08-26 14:41 emielsebastiaan: davxy afk -> 28 Aug: https://github.com/w3f/jamtestvectors/pull/55#issuecomment-3224458898
2025-08-26 15:04 emielsebastiaan: https://github.com/paritytech/polkajam-releases/releases/tag/v0.1.25

CoreVM & DOOM included.  [edited]
2025-08-26 18:48 prematurata: <del>~Hello can someone provide tracelog for storage 13 in 0.7.0? I am struggling to pass that one~</del> [edited]
2025-08-27 14:10 yu2c: davxy afk -> 28 Aug: 
I'm curious about some issues of the jam-test-vector/stf/assurances (v0.7.0)
From [README](https://github.com/davxy/jam-test-vectors/blob/master/stf/assurances/README.md):
1. the "[assurances_for_stale_report-1](https://github.com/davxy/jam-test-vectors/blob/master/stf/assurances/tiny/assurances_for_stale_report-1.json)" pins red dot (which implies error code exists), but in the file, there's no error code but has ok with reports (which implies green dot)

2. Why "[no_assurances_with_stale_report-1](https://github.com/davxy/jam-test-vectors/blob/master/stf/assurances/tiny/no_assurances_with_stale_report-1.json)" doesn't have output? (we don't output timeout reports?)
> Stale work report assignment is removed (but not returned in the output).
2025-08-28 15:42 davxy: I'll have a look. Please open an issue if you want to be sure I don't forget :-D 
2025-08-28 15:50 davxy: Interesting dispute (0.7.0): https://github.com/davxy/jam-conformance/discussions/37
2025-08-28 15:54 prematurata: i will fix the build script for the target tonight but i can give it a spin  on this trace beforehead
2025-08-28 21:35 sourabhniyogi: https://github.com/davxy/jam-conformance/discussions/37#discussioncomment-14249768
2025-08-28 21:35 sourabhniyogi: Jason | JavaJAM: What do you think of the above ^^
2025-08-28 21:45 jaymansfield: I did see that as well but its hard to interpret if its referring to a single parallel accumulation since its referring to **n** and **m** and they are defined in âˆ†âˆ—, or over multiple outer accumulations. [edited]
2025-08-28 21:48 sourabhniyogi: The bottom line is that we cannot accumulate any service more than once.  If you incorporate this constraint, you should be able to match their state root in this case.
2025-08-28 21:50 jaymansfield: You may be right it does make sense but would be good to have davxy's opinion. [edited]
2025-08-28 21:55 dave: image.png
2025-08-28 21:55 dave: Are you referring to this section of the GP?
2025-08-28 21:56 sourabhniyogi: Yes -- does that constraint get modeled in another equation somehow?
2025-08-28 21:56 dave: This is talking about service modification/creation/deletion. It doesn't have anything to do with accumulating a service multiple times AFAIK (which is permitted AFAIK).
2025-08-28 21:57 sourabhniyogi: I am interpreting "altered" as service modification via accumulation.  What blocks the third report from accumulating otherwise? [edited]
2025-08-28 21:58 dave: The point is that the parallel accumulation won't work sensibly if the same service is created/modified/deleted by multiple different services in the same parallel accumulation step
2025-08-28 22:00 dave: As there is no logic to sensibly merge the results
2025-08-28 22:00 sourabhniyogi: Here we have two outer accumulates.
2025-08-28 22:00 dave: Most of that paragraph is merely "informative"; I think the only "normative" bit is that if there _is_ a conflict the block must be considered invalid and discarded.
2025-08-28 22:01 dave: This can only happen if there is a collision amongst the pseudo-randomly generated IDs of newly created services
2025-08-28 22:02 dave: Right, from the perspective of that paragraph the "outer" accumulates should not interact
2025-08-28 22:02 sourabhniyogi: Ok then we need to find the source in the GP which says "If you accumulated service X in one outer accumulate, you cannot accumulate that service X again (in a following outer accumulate) " --  [edited]
2025-08-28 22:02 sourabhniyogi: If there is no source, then Polkajam has a bug, I think.
2025-08-28 22:02 dave: I don't believe there is such a requirement
2025-08-28 22:03 jaymansfield: There might not be one. It could just be a bug. [edited]
2025-08-28 22:04 dave: Yes, quite possibly. I haven't looked into the fuzzer case so I can't comment on what's actually happening there
2025-08-28 22:04 sourabhniyogi: Alright, we'll hold off on publishing a "fix" based on your read =)
2025-08-28 22:09 dave: You shouldn't take my word for it. But I will say that in general most things in the GP are defined formally. You should avoid reading too much into the exact wording of the text surrounding the formal definitions; that is there primarily to aid understanding. Of course if there is a significant mismatch between the two that can and should be fixed...
2025-08-28 22:17 sourabhniyogi: Not sure if this should be in this room or not (you decide!) -- we are trying out running DOOM work packages from the 0.7.0 release of polkajam in a 5+1 polkajam+jamduna testnet  and only seeing "null-authorizer" work packages being submitted by the 2-3 key steps

```
# Create CoreVM.
./jamt vm new ./doom.corevm 1000000000

# Run CoreVM builder (SERVICE_ID is in jamt's output).
./corevm-builder --temp --chain dev --gas 1000000000 SERVICE_ID

# Run CoreVM monitor (SERVICE_ID is in jamt's output).
./corevm-monitor SERVICE_ID
```

where the `corevm-monitor` is showing nothing on the 3rd step and the only work package logging we see is from "null-authorizer" caused by the `corevm-builder`.

Am I missing a key step here or is there something else involved to have us test our JAM  implementations against the famous 60fps target? [edited]
  â†³ 2025-08-29 04:59 clearloop: aint coreVM has different entrypoint that either refine or accumulate will not be able to be invoked [edited]
  â†³ 2025-08-29 05:00 jan: CoreVM payloads cannot be loaded as toplevel services. They use a different blob format than JAM services do, and need hostcalls which are not provided by the JAM client.
  â†³ 2025-08-29 05:12 clearloop: oh I got it, so in this test, `jamt vm ./doom.corevm` creates a process which running the `corevm` and keep submitting work packages to the network
2025-08-28 22:21 dave: I don't think anything is missing there. It does take a little while to get going. `jamtop` should show you the doom service and packages being refined/accumulated for it
2025-08-28 22:22 sourabhniyogi: A little while = how many seconds?  Ok, will run `jamtop`
2025-08-28 22:22 dave: Well, nothing is missing assuming you have a JAM network running
2025-08-28 22:22 dave: If you are not starting a JAM network before those steps then not much will happen :P
2025-08-28 22:23 sourabhniyogi: How many "frames" are represented in each doom work package 
2025-08-28 22:28 dave: I guess 360 if it's running at 60fps. Can't confirm if it actually is running at 60fps as I don't see any indication of the fps anywhere...
2025-08-28 22:29 dave: I can confirm though that it's running for me with the above commands, preceded by `polkajam-testnet` to launch a network [edited]
2025-08-28 22:29 dave: Using the latest nightly release (https://github.com/paritytech/polkajam-releases/releases/tag/nightly-2025-08-28)
2025-08-28 22:30 dave: There are a couple of sticking points though. If you run the `jamt` command too soon after starting the network you will get an error like "2025-08-28 23:24:28 Fatal error: ErrorObject { code: ServerError(1000), message: "Chain error: storage access error: core assigments error: invalid epoch 287320, reference epoch is 0", data: None }

Caused by:
    ErrorObject { code: ServerError(1000), message: "Chain error: storage access error: core assigments error: invalid epoch 287320, reference epoch is 0", data: None }"
2025-08-28 22:31 dave: I also got a "Step error: Work error: BadCode" from corevm-builder the first time I ran it... I don't know what the explanation for that one is. I reran it and it worked the second time! ðŸ˜…
2025-08-28 22:40 sourabhniyogi: image.png
2025-08-28 22:40 sourabhniyogi: Ok thank you for making this happen -- super psyched!
2025-08-29 02:41 sourabhniyogi: David Emett: In attempting 0.7.0 work report alignment between polkajam and our jamduna client (now with a lot of jam-conformance hardening!) against the latest release of `polkajam` (2025-08-28), we believe the authorization args (\[now just E\_2(c)\] (https://graypaper.fluffylabs.dev/#/38c4e62/2ec2002ec200?v=0.7.0)) are incorrect, because very early in the polkajam logs for first invocation of the bootstrap service (in `./jamt vm new ./doom.corevm 1000000000`) we see

```
sp = 0xfefdfa58
u64 [0xfefdfff8] = ra = 0xffff0000
u64 [0xfefdfff0] = s0 = 0x0
u64 [0xfefdffe8] = s1 = 0x0
u64 [0xfefdfc68] = a1 = 0x1 <<<<< the argument input length should be 2, not 1 for 0.7.0 Authorization
```

If we change our encoding from E\_2(c) to E\_1(c) we get work reports to match (in addition to matching this little "a1") ... except for gas where we're wondering when polkajam 0.7.0 does "basic block" gas accounting and when it does single instruction gas accounting. [edited]
2025-08-29 04:32 davxy: You can accumulate one service multiple times. I don't understand why you thought this wasn't possible and came to that conclusion.

The paragraph you shared is completely unrelated - as David mentioned, it refers to conflicts in IDs of new services.

I suspect this might just be a bug on our side. I'll investigate it today. [edited]
2025-08-29 05:07 davxy: There's a bug
2025-08-29 10:06 sourabhniyogi: It had a lot to do with this trace ðŸ˜‚ but really ... because service designers want to have accumulation bring together async refined data together across multiple cores synchronously.    I definitely have a "GROUP BY" map-reduce view of what I want to see happen from JAM's 'rollup host' purpose:  A service designer will want to integrate ALL the work that was done in the last N seconds every N seconds and have a single place to compute and write out a SINGLE view of what happened.

While I'm glad no work report is left behind, JAM's accumulate appears to have a depressing G\_T problem for its rollup host purpose:  accumulate does NOT accumulate everything every N seconds, not even typically.  Yes, you can organize work packages in chain with every work package having a prereq, but that is pretty lame. [edited]
2025-08-29 10:09 dave: I believe the only case where JAM _won't_ accumulate everything is when there are unsatisfied dependencies
2025-08-29 10:10 dave: It will accumulate these as their dependencies are satisfied, subject to per-block gas limits
2025-08-29 10:11 dave: So I'm not sure I agree with "it won't typically accumulate everything"
2025-08-29 10:15 sourabhniyogi: I mean a service designer would like to see *one* `accumulate` execution across all work reports.  Whether it typically does or not depends on what else is happening.  Its definitely not the case that a service designer can expect to get all the work reports grouped together.  Anyway, we're outside the topic of JAM conformance =)
2025-08-29 10:16 dave: You certainly can't rely on that no
2025-08-29 14:15 boymaas: davxy:  I see you are already publishing the import stats. That's great! I was wondering if you are measuring with verbose output enabled, or if you ran the JamZigâš¡ node with -vv or without any verbose flags. I compiled these with detailed runtime debugging on, and the best performance is achieved without -vv and with all debug output removed. Let me know if you want a new binary compiled without this.
2025-08-29 14:23 davxy: I'll disable debug then. No worries -- Zig looks fast ;-)  
2025-08-29 14:24 davxy: FYI, we're working on extracting some rough numbers to get a basic comparison between the implementations and to provide a bit of extra incentive :-D  
Soon we'll publish a nice-looking chart, courtesy of Erin.  

The perf reports are based on monitoring your import timings (accounting for the small overhead of the process link). Nothing scientific here -- just estimates.  
In practice, we also run polkajam as a target. In this case, the difference between the fuzzer importing the block and the target import is pure overhead.  This is the overhead that we considered for your targets as well.
The perf reports are generated from the test vector traces.  

I'm scripting the production of these numbers, so if you update your binary, regeneration will be immediate.

  â†³ 2025-08-29 17:53 clearloop: hi davxy just did some optimizations! hope can try spacejam again! https://github.com/davxy/jam-conformance/issues/13#issuecomment-3237780866
2025-08-29 14:34 clearloop: curious about what's the system info of the benchmarking machine, also, if it is possible providing memory peeks as well, I'm a bit worry about it since some programs are pretty big [edited]
2025-08-29 14:39 davxy: Perhaps we can run this on some more beefy machine at some point.
But for the moment I think these results already gives you enough hints 
2025-08-29 14:39 jaymansfield: I published a new version of javajam yesterday that included about a 70% import performance improvement for the storage test vectors. Are you able to grab the latest 0.2.7? It looks like the report used 0.2.6.
2025-08-29 14:39 davxy: My machine 
AMD Ryzen Threadripper 3970X 32-Core (64) @ 4.55 GHz
2025-08-29 14:40 davxy: > it is possible providing memory peeks as well

Current reports not, perhaps we will in the future. 
2025-08-29 14:46 davxy: done
2025-08-29 14:50 jaymansfield: Thanks!
2025-08-29 14:51 ascriv: Where can I see the import stats? Been afk for a while 
2025-08-29 14:53 ascriv: Or are they not published yet
2025-08-29 14:54 clearloop: Jamzilla is super fast 

https://github.com/davxy/jam-conformance/blob/import-perf/fuzz-reports/0.7.0/reports/jamzilla/perf/storage.json
2025-08-29 14:57 ascriv: ðŸ«¢
2025-08-29 14:59 clearloop: I'm curious are you all running accumulation in parallel already, why in that speed
2025-08-29 15:00 ascriv: I am doing in parallel yes
2025-08-29 15:00 ascriv: Interpreted pvm for now 
2025-08-29 15:00 sourabhniyogi: davxy: Which metric should teams optimize for to get the most fuzzing action?  

```
% grep import_p75 */perf/storage.json  
boka/perf/storage.json:    "import_p75": 6925.978,
jamduna/perf/storage.json:    "import_p75": 171.9,
jamzig/perf/storage.json:    "import_p75": 39.349,
jamzilla/perf/storage.json:    "import_p75": 81.848,
javajam/perf/storage.json:    "import_p75": 233.967,
pyjamaz/perf/storage.json:    "import_p75": 5186.699,
spacejam/perf/storage.json:    "import_p75": 187.597,
turbojam/perf/storage.json:    "import_p75": 10.395,

```
  â†³ 2025-08-29 15:02 clearloop: can't believe the performance of turbojam
  â†³ 2025-08-29 15:04 davxy: You have two ways of make the fuzzer happy:
- run the STF
- hardcode the replies in the binary :-D (given that we run the test vectors traces here)
  â†³ 2025-08-29 15:04 sourabhniyogi: Removing logs and recompiler is 95% of the optimizing
  â†³ 2025-08-29 15:04 davxy: But the thing is that turbojam has the same perfs with a random seed. So it can't predict the state roots. Ergo. Looks really fast
  â†³ 2025-08-29 15:05 davxy: I'll provide perf reports for polkajam in few mins
  â†³ 2025-08-29 15:05 davxy: both with interpreter and recompiler
  â†³ 2025-08-29 15:05 davxy: you can provide optimized binaries. I don't need the logs actually
  â†³ 2025-08-29 15:07 davxy: @room If you republish optimized bins, please leave a note in your dedicated GH issue.  
I wonâ€™t re-run them right away, and otherwise Iâ€™ll probably forget.

  â†³ 2025-08-29 15:09 clearloop: I doubt about the performance of recompiler... I've tried compiling the programs in the stroage set however they at least take 10s on compilation... not sure what's the number from others [edited]
  â†³ 2025-08-29 15:10 jan: 10s per program or in total?
  â†³ 2025-08-29 15:11 clearloop: per...I must made sort of horrible mistake... haven't implemented the cache yet so each test triggers a compilation, there mb sort of memory issues as well

still reorganizing my implementations these days, will take the benchmark with polkavm/bentools soon! [edited]
  â†³ 2025-08-29 15:14 jan: Note that if you have optimal enough code then you don't actually need a cache. Anyway, 10s seconds sounds very abnormal.
  â†³ 2025-08-29 23:20 clearloop: Just made my recompiler 20x faster via removing my "sugar" of register operations .... compiling storage\_light::00000004 from `11s -> 0.6s` indeed horrible mistake! trying to remove more "sugar methods" now XD I was actually handling the instructions with a interpreter mind! [edited]
2025-08-29 15:01 boymaas: davxy:I pushed a new version for JamZigâš¡ with all the debugging mechanics removed. I'm curious about the performance gains! It looks very promising locally. ðŸ¤  The -vv flags on the target do not do anything anymore.
2025-08-29 15:20 boymaas: https://asciinema.org/a/eMIt8geWsubdseBLBn9ygUkiN ðŸ¤ 
  â†³ 2025-08-29 15:23 clearloop: this visualization tool looks perfect, can you make a PR to the repo that we can try it as well?
2025-08-29 15:21 boymaas: image.png
2025-08-29 15:22 jan: Would be nice to also have the implementation language next to the name.
2025-08-29 15:24 dakkk: is this using only fallback / safrole right?
2025-08-29 15:24 boymaas: Safrole indeed
2025-08-29 15:26 dakkk: davxy: I managed to solve the issue that was causing an error on jampy when using the pvm; I've just uploaded a fixed version
2025-08-29 15:38 boymaas: Good idea, let me check if I can add that quickly.
2025-08-29 15:41 sourabhniyogi: Way to go everyone -- this will turbocharge all of us =)
2025-08-29 15:43 boymaas: Does anyone know what language TurboJam uses? I cannot find it that quickly online.
  â†³ 2025-08-29 15:43 clearloop: C++
2025-08-29 15:44 emielsebastiaan: davxy: Is there a reason why you choose not to include `preimages` & `preimages_light` in the speed benchmark?
2025-08-29 15:44 arjanz: C++ I believe
2025-08-29 15:44 ascriv: The repo is public too
2025-08-29 15:44 davxy: Not really. I can add in the next iteration
2025-08-29 15:48 prematurata: wow nice charts. :)
2025-08-29 15:49 prematurata: do we know all language sets? it would be nice to have it after the name until we get used to it. I, for example dont know many of other implementors languages other than the ones that contain it in the name
2025-08-29 15:50 dakkk: it could be useful to have a script that generates those charts automatically for every test set
2025-08-29 15:52 boymaas: image.png
2025-08-29 15:53 davxy: Added polkajam and polkajam iterpreted
  â†³ 2025-08-29 16:03 clearloop: fact: polkajam is 2x faster than turbojam = =
2025-08-29 16:00 davxy: Given the data I have provided, you do not need to worry about variations of around +/-3ms.  
This is not a dedicated machine, so results may fluctuate a bit.  
Still, the data gives good hints on where improvements are needed, and it shows which trace step took more time so you can re-execute it.

2025-08-29 16:00 davxy: https://github.com/davxy/jam-conformance/tree/import-perf/fuzz-reports/0.7.0/reports/perf
2025-08-29 16:04 boymaas: image.png
2025-08-29 16:04 boymaas: Forgive me if I have associated the wrong language; please let me know, and I will correct it.
2025-08-29 16:08 ascriv: Turbojam is c++, not unknown
   https://github.com/r2rationality/turbojam
2025-08-29 16:10 sourabhniyogi: Ok, since polkajam is at the top 3 of the `reports/perf` (for non-zero values anyway), we can all aim to turbocharge up to polkajam `polkajam_perf_int` most easily if we use polkajam fuzzer target to compare against our own.  

Is there a fuzzer option within `polkajam` or is that a separate binary that can be provided?

Not asking for polkajam fuzzer binary -- just the polkajam fuzzer target.
2025-08-29 16:12 jaymansfield: Will be interesting to see where everyone stands in a few days after working on optimizations
2025-08-29 16:12 jaymansfield: Really useful seeing the numbers
2025-08-29 16:12 boymaas: Thank you ascriv | Jamzilla and arjan | PyJAMaz  will add it.
2025-08-29 16:47 danicuki: Jamixir already has 0.7.0 - would you please include it in this speed report?
2025-08-30 07:02 dakkk: > <@boymaas:matrix.org> Forgive me if I have associated the wrong language; please let me know, and I will correct it.

Can you share the script you re using to create this chart? 
2025-08-30 07:40 boymaas: visualize_perf_enhanced.py
2025-08-30 08:32 boymaas: davxy: Feel free to include it in the repo as well.
2025-08-30 10:56 davxy: Can you open a PR ? 
2025-08-30 10:56 dakkk: maybe you can also integrate it in the github-CI, so every time perf are updated, it creates the chart
2025-08-30 10:57 prematurata: this changed from "make a graypaper compliant implementation" to a drag race [edited]
2025-08-30 10:57 prematurata: :)
2025-08-30 10:57 dakkk: image.png
2025-08-30 10:57 dakkk: image.png
2025-08-30 10:57 dakkk: btw, those are from last update: 
2025-08-30 10:58 prematurata: tsjam is TypeScript Language btw... i see it is marked as unknown [edited]
2025-08-30 11:04 jan: Jamixir is Elixir AFAIX (hence the "ixir" in the name)
2025-08-30 11:06 jan: And it'd make more sense to change "Rust (int)" to "Rust" and "Rust" to "Rust (recomp)" for polkajam (or perhaps have another column whether the implementation is using a recompiler; not sure if any other implementation besides polkajam uses one currently?) [edited]
2025-08-30 11:07 ascriv:  I think jamduna has one?
2025-08-30 11:07 ascriv: @sourabhniyogi 
2025-08-30 11:08 jan: Yeah, I know a few of the implementations have one in-progress, but it's unclear if any are used here for these tests. [edited]
2025-08-30 11:09 clearloop: btw I want to confirm if block based gas charging matches the tracing tests, we can match accumulate tests but not tracing tests with block based gas charging, could be caused by the host call tried once but seems the problem is not that obvious [edited]
2025-08-30 11:10 jan: Block based gas is not yet part of the GP; not sure if current traces use per-instruction gas yet (you'd have to ask davxy ), but I have implemented per-instruction gas metering in PolkaVM so the plan is to have the fuzzer/traces use per-instruction gas until the new gas cost model is ready.
2025-08-30 11:11 jan: You can already start implementing a block-based gas cost model to get a head start on the final gas cost model, but for now the official model is per-instruction.
2025-08-30 11:18 davxy: We use per-instruction gas charging. The test vector traces work for both models, since they never fail midway through a block.
2025-08-30 11:18 clearloop: kk so the problem is in my implementation, will take a deeper look at it then!
2025-08-30 11:55 danicuki: > <@prematurata:matrix.org> this turned out from "make a graypaper compliant implementation" to a drag race

Tell me how you measure me and I will tell you how I will behave. 
2025-08-30 13:46 davxy: EoW batch: https://github.com/davxy/jam-conformance/pull/41 
2025-08-30 13:47 davxy: 0.7.0 table:
https://github.com/davxy/jam-conformance/blob/reports-batch-0.7.0/fuzz-reports/README.md#disputes
2025-08-30 14:55 boymaas: dakkk | JamPy: If you updated the script, please feel free to create the PR for the visualize_perf tool.
2025-08-30 17:03 vinsystems: I started a discussion about trace 1756548916 https://github.com/davxy/jam-conformance/discussions/42 which affects almost all teams 
2025-08-30 17:23 davxy: replied
2025-08-31 10:39 danicuki: davxy: how frequent do you update the reports tables - (performance and disputes)? On our side, do we need to notify you when we have a new version of our binary, or just put the binary in same place? 
2025-08-31 12:55 davxy: In general, just drop a message in your team's issue.  
I usually re-run the scripts when it is worthwhile -- for example, if a team has made significant improvements or fixed a some traces.  

Of course, do not expect me to re-run them immediately after a ping, since I might be away or focused on something at the time.  

Still, given that there are not too many teams, I tend to run the scripts more frequently than expected, as I enjoy seeing things improve and become more resilient to the fuzzer day by day.

2025-08-31 13:32 davxy: Just to be clear - currently - the performance table is mostly meant to provide an indication of how long fuzzing should run to reach the same level of confidence across implementations. This is also to start thinking a bit more about the auditing process duration.

For example, if we take (in the screenshot shared by dakk above) PolkaJam as the baseline, and auditors require 3 days of continuous fuzzing without crashes/diffs then:

- An implementation 300x slower would require 900 days (about 2.5 years)
- If the baseline is instead 10x slower than Polkajam, the same 300x implementation would need 90 days

These numbers are speculative, but that is the point of sharing performance results at this stage: they are not about improving by few ms to climb the ranking, but about giving a tangible sense of scale for testing.

I will likely update performance numbers less often (about once per week) to reduce noise from micro-optimizations and keep the focus on M1 compliance. Slower implementations that report tangible improvements may get more frequent updates. [edited]
2025-08-31 13:39 jaymansfield: I have started a discussion about 1756548741 : https://github.com/davxy/jam-conformance/discussions/43
2025-08-31 17:37 jaymansfield: Regenerated the rankings:
2025-08-31 17:37 jaymansfield: Screenshot 2025-08-31 at 1.34.03â€¯PM.png
2025-08-31 17:37 jaymansfield: Screenshot 2025-08-31 at 1.35.38â€¯PM.png
2025-08-31 17:37 jaymansfield: Screenshot 2025-08-31 at 1.35.48â€¯PM.png
2025-08-31 17:41 clearloop: seems our perf is not updated XD, we should be at least ~3x faster now
2025-08-31 19:01 sourabhniyogi: Since most of us are deathly afraid of flaming out of fear of being too slow (or have really large egos ðŸ¤ªðŸ¤£ ), why not just state some precise guidelines now that you got almost all of us into basic "walking" shape.

Concretely:

1. Your implementation _should_ perform no slower than 10x (or 25x or 50x, you pick the number) polkajam\_int across all test groups
2. Your interpreter _should_ be able to interpret at 1MM gas/s (or 10M gas/s, 50MM gas/s, you pick the number }.

By stating this clearly, teams who are way ahead of this "should" threshold can proceed without fear on winning the M3/M4 marathon (refining Doom/.. workpackages with a recompiler) rather than an M1 sprint rankings game (which we should save for M3/M4, right?).  In the end the timings are dominated by PVM accumulate execution?

What do you think -- does that make sense? [edited]
2025-08-31 19:11 dakkk: > <@sourabhniyogi:matrix.org> Since most of us are deathly afraid of flaming out of fear of being too slow (or have really large egos ðŸ¤ªðŸ¤£ ), why not just state some precise guidelines now that you got almost all of us into basic "walking" shape.
> 
> Concretely:
> 
> 1. Your implementation _should_ perform no slower than 10x (or 25x or 50x, you pick the number) polkajam\_int across all test groups
> 2. Your interpreter _should_ be able to interpret at 1MM gas/s (or 10M gas/s, 50MM gas/s, you pick the number }.
> 
> By stating this clearly, teams who are way ahead of this "should" threshold can proceed without fear on winning the M3/M4 marathon (refining Doom/.. workpackages with a recompiler) rather than an M1 sprint rankings game (which we should save for M3/M4, right?).  In the end the timings are dominated by PVM accumulate execution?
> 
> What do you think -- does that make sense?

I dont think it would be a good idea for M1 and M2, interpeted languages are slower by design, and they may never achieve certain performances. [edited]
2025-08-31 19:12 sourabhniyogi: image.png
2025-08-31 19:12 sourabhniyogi: That makes sense -- probably need to make Set C + D versions of those guidelines based on the data.
2025-08-31 19:15 sourabhniyogi: Since you (and other Set C folks) know what Python (and other interpreted languages) can achieve, what would you have as guidelines?  Key word is "should" not turning into a "must" [edited]
2025-08-31 19:19 dakkk: Idk, I personally prefer not to have constraints on it
2025-08-31 19:19 sourabhniyogi: Using the word "must" would be a constraint.  "should" would mean you would be expected to improve your interpreter to get more fuzzing action.
2025-08-31 19:27 davxy: What you said makes sense, but keep in mind:

1. The data we collected is meant to support this goal. Giving guidelines without data would just be speculation.

2. I can't really give you an answer here. You might need to escalate this kind of question to the architect :-D Maybe in jam chat. Not sure.
2025-08-31 19:31 davxy: I see that Gav is not in this channel BTW
2025-08-31 19:32 clearloop: I'd like to share some opinions at this topic as well, not sure if we should pivot this discussion to the let's jam channel first [edited]
2025-08-31 19:32 davxy: So yeah, if your questions can't be answered here, I'd say try with the JAM channel
2025-08-31 19:34 clearloop: could you please (mb) re-post your paragraph in the let's JAM channel sourabhniyogi , and then we can continue on it? [edited]
2025-08-31 20:00 prematurata: any difference in tiny for WA and WB and WC? [edited]
2025-09-01 07:02 prematurata: I want to tell you how i lost 4 hours of my life debugging. but i will share the code. now that I share you'll instantly find it but it took me 4 hours : `SERVICECODE_MAX_SIZE = 4_000_0000;`

damn fat fingers [edited]
2025-09-01 07:37 clearloop: I used to spend 2 days for an encoding order error = =
2025-09-01 07:39 davxy: Jason | JavaJAM: can you pls open a PR with the latest version of the perf script? I'd like to have it in the jam-conformance `scripts` folder. Ty
2025-09-01 08:09 clearloop: I just made one for Boy Maas | JamZigâš¡ with his commit info, would you mind checking it over again or you can make one instead Boy Maas | JamZigâš¡ , wanna keep you as the original author of this script in the git history anyway for respect https://github.com/davxy/jam-conformance/pull/46 then we can join the update of it together later [edited]
2025-09-01 08:31 boymaas: Thank you clearloop | SpaceJam  I asked you to create one. Thank you for the PR and the attribution; I appreciate it! ðŸ˜Ž
2025-09-01 09:47 davxy: Please add your considerations/proposals
2025-09-01 09:47 davxy: https://github.com/davxy/jam-conformance/pull/47
2025-09-01 10:03 sourabhniyogi: Jason | JavaJAM: would you like to add your ideas on how to add refinement to the above?  davxy is this ok to do?
2025-09-01 10:05 sourabhniyogi: Are there \[Set C/D?\] teams who want to work on an FFI into a PVM recompiler?  If so, we probably can do a meaningful call together, well beyond intros, at the intersection of everything. [edited]
2025-09-01 10:34 bamzedev: davxy: https://github.com/davxy/jam-test-vectors/blob/master/stf/reports/tiny/banned\_validator\_guarantee-1.json if I understand correctly, you are expecting that the guarantee is invalid, because one of the guarantors is in the offenders list? If that is the case, could you point out to the GP equation for that validation? [edited]
2025-09-01 11:54 davxy: 11.21 & 11.22
https://graypaper.fluffylabs.dev/#/1c979cb/155800155800?v=0.7.1 
key is zeroed out if belongs to the offenders set (as per 6.14)
2025-09-01 11:55 davxy: I'll add it to the features. 
2025-09-01 12:03 sourabhniyogi: would you be able to join a call with us (this week or next) to give us some tips on how to do a refine fuzzer that is approximately what you would do?   I understand your priorities are on M1 but a little guidance would go a long way
2025-09-01 20:44 sourabhniyogi: https://github.com/davxy/jam-conformance/pull/50
2025-09-02 00:20 r2rtnl: A discussion about 1756548706: https://github.com/davxy/jam-conformance/discussions/51
2025-09-02 09:20 davxy: Reports batch: https://github.com/davxy/jam-conformance/pull/52
2025-09-02 12:03 ascriv: @davxy can you enlighten us on what is left to be done in pre-submission fuzzing before M1 submissions are opened?
2025-09-02 12:07 davxy: 1. finish some open tasks on the fuzzer 
2. define/enact fuzz protocol v1 https://github.com/davxy/jam-conformance/pull/47 
3. better define baseline timings/requirements/auditors etc. I'll discuss about this with the team as soon as I have the opportunity

2025-09-02 12:09 ascriv: thanks!
2025-09-02 15:55 boymaas: 
2025-09-03 12:00 erin: Hello everyone, the conformance performance dashboard is now ready -> https://paritytech.github.io/jam-conformance-dashboard/

Please let me know if anything looks off/wrong. Do note it does use a weighted scoring mechanism (methodology detailed at the bottom of the page) so it may not always look strictly "in order". [edited]
2025-09-03 12:07 erin: Checks for new data are performed hourly.
2025-09-03 12:38 oliver.tale-yazdi: Maybe there could be an option to anchor it to a "fixed baseline" version of PolkaJAM (e.g. 0.7.0)?  
Then teams can continuously check if they improved, otherwise it would look like they get slower after PolkaJAM optimized a bit [edited]
  â†³ 2025-09-03 12:41 erin: good feedback, thanks!
2025-09-03 18:35 boymaas: Very nice dashboard erin . Now I also need to start optimizing ðŸ¤ 
2025-09-03 19:15 clearloop: wow, this dashboard looks much more prettier than I can imagine [edited]
2025-09-05 11:00 sourabhniyogi: For all of us macro data refiners (you know you want to =)), please check out / comment on https://github.com/davxy/jam-conformance/pull/59 -- this aims to add the ability to trace the causes of work report differences with a new `feature-exports` in addition to the basics of `feature-bundle-refinement`.  
2025-09-05 20:30 jaymansfield: https://github.com/davxy/jam-conformance/discussions/64
2025-09-08 10:01 dakkk: davxy:  what should we do when we create a new release of our impl? I see other teams tags you in GH issues both for updating perf and traces. Is it ok for you?
It could be an idea to schedule an automatic rebuild of reports / perf (once or twice a day)

2025-09-08 11:42 boymaas: image.png
2025-09-08 11:42 boymaas: Good question. I started optimizing yesterday, and now a new version is coming with some promising improvements. It's not done yet. ðŸ¤  [edited]
2025-09-08 16:46 davxy: Feel free to leave a note  in your team's issue.  
For now, though, I'm not going to regenerate performance reports more than once a week.

Automation would be nice, but I'm not going to invest my time improving my hacky scripts and maintain some infrastructure.  
I realize automation may seem like a "fire and forget" solution, but right now that is not feasible.  

On top of that, my DevOps skills are not sharp enough to make the setup fully robust :-D

Last but not least, the focus should stay on M1, not on chasing a tight race for the top spot in the speed rankings.


2025-09-08 17:02 sourabhniyogi: For [Fuzzer Protocol v1](https://github.com/davxy/jam-conformance/pull/47) should we aim for implementing that this week or next amidst v0.7.0 or plan to have that as part of v0.7.1? 


2025-09-08 17:03 sourabhniyogi: Probably some timing expectations on when we wrap up 0.7.0 fuzzing and do 0.7.1 would be useful?
2025-09-08 17:07 sourabhniyogi: Not sure if we should be doing { Ancestry support, Simple forking } exploration in 0.7.0 fuzzing  -- it may be simplest for everyone to have Fuzzer Protocol v1 synced with 0.7.1?
2025-09-08 17:19 rustybot: I think we are ready to deploy v1. Is just about changing the PeerInfo message, which is trivial. If a target doesn't support a feature can turn off the flag
2025-09-08 17:22 rustybot: https://github.com/davxy/jam-conformance/pull/47#issuecomment-3267172222
2025-09-08 17:26 sourabhniyogi: How about finalizing v1 by merging 47 (with or without an Error string) and having a specific date (suggest: 9/11) be the v0=>v1 cutover?
2025-09-08 17:35 davxy: Sounds good. What about the Error message? Sounds good? Any suggestion? I'd like to keep this dead simple
2025-09-08 17:36 davxy: So perhaps the error message (which can be empty) can be included in the fuzz report
2025-09-08 17:42 sourabhniyogi: The situation is that because you give us 1-5 homework disputes every 2-3 days, it makes very little difference since we just debug each of them by hand.  If it was 10-100x greater and we had "I can't replicate what you see" problems an error string would matter a LOT!   But we do not. However, I imagine if you had programmatic fuzzing (of PVM) working at 10-100x scale, these error strings will be 10-100x more useful for fuzzing scalability.

I think a lot of us became SUPER performance focussed in the last 7-10 days because of the "audit" expectations that we will be doing 10-1000x more fuzzing, running for 3 days rather than 3-30s.  The moment you have programmatic fuzzing working at higher scale, it becomes valuable.  Looking ahead to PVM fuzzing, this error string could summarize the result of "we differ at gas/step/opcode with xyz registers" from the fuzzer probing the target via this feature https://github.com/davxy/jam-conformance/pull/65 -- but we should not have this feature part of v1 with a v0=>v1 of this week. [edited]
  â†³ 2025-09-09 01:20 clearloop: I got sick last week on working on my implementation, (after 3~5 hours sleep per day) for re-writing my compiler, and my compiler is now getting even worse ðŸ˜…
  â†³ 2025-09-09 02:32 xlchen: this is the fun part of doing optimizations. you never know if your optimization actually optimizes it or not until you spend/wasted bunch time on it [edited]
  â†³ 2025-09-09 19:30 danicuki: > <@xlchen:matrix.org> this is the fun part of doing optimizations. you never know if your optimization actually optimizes it or not until you spend/wasted bunch time on it

In practice, the theory is different. 
2025-09-08 18:23 sourabhniyogi: In the end, because there is just 1 of you and 15+ of us, we should do whatever makes your life easier so we get over the finish line together faster -- anything to increase the fuzzing level so we get as much conformance as possible and at least half of us done with M1 before sub0 ( ideally ) and most of us before xmas would be best.  It seems pretty clear from the extremely high speeds that teams do their homework each week that we're all capable and motivated to work at high speeds but we don't want to stress you out either =) [edited]
2025-09-09 01:10 clearloop: agree with the error message, because the current report (per team level), doesn't make a lot of sense since we can get them any way on debugging them locally (diff of key-values), with error messages embedded, the reports will be more useful, and it would be nice if the fuzzer provides error messages in the report as well, e.g. for the cases the the target can import the blocks while the blocks should not be imported

from yesterday till today, I have met some traces broken remote but works locally, and without the error message from the remote, I actually have no ideas what to fix ... [edited]
2025-09-09 14:45 davxy: https://github.com/davxy/jam-conformance/pull/68
2025-09-09 20:47 vinsystems:  https://github.com/davxy/jam-conformance/discussions/71 
2025-09-09 20:53 prematurata: damn bernar. you beat me by some minutes. i opened it as well for 771 https://github.com/davxy/jam-conformance/discussions/72
2025-09-09 21:35 davxy: Fuzzer protocol v1 will go into effect on Friday, 12 Sep. Starting on that date, our fuzzer will implement the changes proposed in:
https://github.com/davxy/jam-conformance/pull/47 [edited]
2025-09-09 21:43 sourabhniyogi: Friday 9/12 yes?
2025-09-09 21:50 davxy: Fixed ðŸ˜…
2025-09-10 10:30 clearloop: ( if the fork tests the last set of M1 ? ^ ^)
2025-09-10 10:41 ascriv: > <@davxy:matrix.org> Fuzzer protocol v1 will go into effect on Friday, 10/11. Starting on that date, our fuzzer will implement the changes proposed in:
> https://github.com/davxy/jam-conformance/pull/47

On import failure due to a jam protocol-defined error condition, it seems like the README says we should both return the 0 hash AND an 'Error' message, but shouldn't it be one or the other?
2025-09-10 10:44 davxy: We should return Error message. I need to remove the 0 hash requirement. That is a leftover
2025-09-10 14:12 clearloop: btw wanna discuss about if either header verification or fork detection should be in the network level? if the received header is not validated, we won't execute it at all (or put it in the pool), tbh the fork part and the header validation breaks our design, that I have to downgrade my validations from my network layer to the block import layer, and for the fork part, I have to rewrite the logic just for tests (bcz there is no finalization, no "grandpa", no peer handshake info)

but as "block importer", header validation & fork detection makes sense again ... [edited]
2025-09-10 14:51 dave: Not entirely sure what you mean. From the networking perspective you should verify blocks before announcing them to peers. You should announce and attempt to import all forks. It should be possible to verify blocks significantly faster than executing them. In particular, it is intended to be possible to verify a block without performing any accumulation, so without any PVM execution. There is _one_ validity condition that cannot be checked without performing accumulation; that we don't attempt to create two services with the same ID. It is not intended that this is checked before announcing/distributing a block to peers.
2025-09-10 14:53 dave: I think we could probably only require verification of the author and the author's signature before announcing, this isn't what the JAM-NP doc currently says though
2025-09-10 14:57 clearloop: > From the networking perspective you should verify blocks before announcing them to peers

yes, but verifying headers after producing a block or received a new block from remote peers which is right happens at the SNP, if it is validated, I'll put it into a fork, waiting for the block processor to pick it up then execute (block import), I meant that **header validation is ahead of block execution**, they are not bound together, also if we receive a block earlier than the finalization chain, I won't even validate it, that for adapting the tests, I need to bind them together, but it's fine indeed since the logic is minor and the header verification logic do need to be confirmed or it may be tough for the networking part [edited]
2025-09-10 15:02 dave: Still not really sure what you're saying. Are you saying the fuzzing interface is forcing you to change your design, even though your current design is fine for implementing JAM-NP?
2025-09-10 15:04 clearloop: sorry and yes I meant that I have to write new logic just for adapting the tests now (which will not contribute to the production), header validation is indeed fine, but testing fork at the current state, I think it may go out of control finally [edited]
2025-09-10 15:14 dave: Nodes are expected to follow all forks, not clear to me why it's an issue for the fuzzer to test forks? What do you mean by "testing fork at the current state"?
2025-09-10 15:20 clearloop: for example, we are now introducing fork tests at this level:  https://github.com/davxy/jam-conformance/discussions/71#discussioncomment-14360978 , simply replace the latest imported block

I believe the approach in production is creating a new fork at parent, results in we now have two fork chains (currently without grandpa or finalization), but for the current fuzzing interface, we are more like just maintaining a "finalized chain" (or importing from a finalized chain), which makes conflict, and starting from this, I'm afraid that we'll introduce more and more parameters/arguments in the fuzzer till we can cover the entire fork system, and during the process, I believe for some teams, their work will be rewriting their implementation 5%, 10%...100%, till they back to the status months ago
 [edited]
2025-09-10 15:42 dave: Ok, think davxy's input is needed here. But I would think you probably shouldn't be immediately finalizing blocks that are imported. Ideally the fuzzer won't force the node to be architected in any particular way, but you're naturally going to have to write _some_ code to interface with it; M1 doesn't require JAM-NP so we can't just use that.
2025-09-10 15:44 clearloop: > But I would think you probably shouldn't be immediately finalizing blocks that are imported.

what do you think about the case for light node? If I connect a node which is present in validator set, can I import its finalized blocks without verification? [edited]
2025-09-10 15:44 dave: Light nodes will probably follow the finalized chain and so lag slightly
2025-09-10 15:47 dave: Though it really depends on the use case, for some use cases maybe it's good enough to trust instead of verify
2025-09-10 15:48 dave: In the case of a finalized block you can always ask for proof of finality
2025-09-10 15:48 dave: (This isn't in the JAM-NP spec, GRANDPA stuff will be added soon)
2025-09-10 15:49 clearloop: but if a node is present in validator set, and we can't trust its finalized blocks, ain't the consensus got broken, or the case would be like,

- if we importing blocks from it from 0 till the latest finalized block, we can trust (mb with small verifications)
- If we just pick sort of blocks in the middle, we'll need the proof of finality

hmm, but the rpc may cheat or broken as well? [edited]
2025-09-10 15:51 dave: What do you mean by "trust its finalized blocks"? Validators are generally fairly trusted but the protocol is designed to work with up to 1/3 byzantine validators. A byzantine validator could lie to you about which blocks have been finalized
2025-09-10 15:52 dave: If you want to be sure a block has been finalized you need to ask for a proof and verify it. A proof will consist of various GRANDPA votes, possibly going all the way back to the genesis block to verify validator set changes and so on
2025-09-10 15:52 dave: RPC is totally different, as the RPC interface is specifically intended to only be used between two processes which trust each other [edited]
2025-09-10 15:53 dave: Probably because they are being run by the same operator
2025-09-10 15:57 danicuki: Sorry if I missed this information in the protocol: when a block should not be imported for any reason, how do we know the expected reason why the block wasn't imported? On test vectors there was an expected error message. Do we have something similar in the fuzzer?
2025-09-10 15:57 clearloop: you got me, probably I have done some work less of security in my previous work lol, e.g. importing blocks from open rpc directly without checks for building centralized databases, the concepts of proof of finalization makes sense to me! [edited]
2025-09-10 16:01 clearloop: I had asked for it before as well, but seems it's still not included in Fuzzer V1 yet (only has error from target to fuzzer but not fuzzer to target), probably we can comment it in https://github.com/davxy/jam-conformance/pull/47 [edited]
2025-09-10 16:33 davxy: Ideally, you should not need to write any special code to interact with the fuzzer.

You only need to implement the communication channel, which is straightforward.

No special handling is required at the block import level.

## Basic Protocol

1. Receive a block
2. Import the block, which yields either success or failure
3. Reply:
    - On success: return the resulting state root
    - On failure: return the `Error` message (the reason is irrelevant and not specified in the GP)

## Handling Forks

In production, you must support forks that occur above the last finalized block.

However, since there is no finalization rule in this context, you will not finalize anything.

Technically, this means you must be able to handle any kind of fork.

For fuzzing, of course, this is not required.

The only requirement (which matches a correct implementation and does not need special code)

is that you must allow forks at the **last imported block**.

This is necessary because fuzzing involves mutating blocks in several ways,

and you must attempt to import all the mutations (some may fail some may be successfull -> fork)

## Example Session

1. Let `i = 0`
2. Increment i and Produce block `B_i` with parent `B_(i-1)`
3. Mutate `B_i` into several variants: `B_i1`, `B_i2`, `B_i3`
4. Send these variants in order: `B_i1`, `B_i2`, `B_i3`, and finally `B_i`
5. Repeat from step 2, using `B_i` as the parent for the next batch

## Notes

- Finality is not required: just import the blocks
- Forks created by the mutations can remain in the state
- (optional) You may prune these forks as you progress if you wish [edited]
2025-09-10 16:34 davxy: This goes straight into the fuzz protocol v1 readme
2025-09-10 16:39 davxy: The trace and report. Re execute to get why your impl fails. 
This is the same as the jam-test-vectors traces, a dynamic that matches how the system actually works. [edited]
2025-09-10 16:43 clearloop: > The only requirement (which matches a correct implementation and does not need special code)
> is that you must allow forks at the last imported block.
> 
> This is necessary because fuzzing involves mutating blocks in several ways,
> and you must attempt to import all the mutations (some may fail some may be successful -> fork)

well I can accept the current fork system now since I just realized that I have to refactor my test infura anyway now..., but want to confirm **if the current fork system won't be expanded anymore?**

block mutation happens, and we have already had the logic which is pretty large, handling orphan blocks, invalid blocks, forks, fork of forks, block mutation may happen in any part of them and we have already handled all of them, but for adapting the fuzzer, we still need to write new logic, because our exist system requires more informations to bootstrap [edited]
2025-09-10 16:46 davxy: > but want to confirm if the current fork system won't be expanded anymore

I can confirm this. At least according to our fuzzer protocol proposal [edited]
2025-09-10 18:21 danicuki: In this case, https://github.com/davxy/jam-conformance/blob/main/fuzz-reports/0.7.0/traces/1757406441/00000117.json - the post_state is same as pre_state. This means that the block import should fail for some reason, resulting in the same state root. But we don't know, based on the trace, what is the reason why you are rejecting the block. On test vectors, there is something like `"err": "bad_validator_index"` or other hints about the reason why the block fails. 
2025-09-10 19:10 davxy: Hmm interesting... indeed we could include the result of our last block import in the final fuzz report - it might serve as a helpful hint.

However, should be noted that the evaluation order of some expressions is arbitrary, so with a block with the potential to trigger multiple errors the result could is a bit ambiguous.


Edit: After thinking about it, I agree - I don't see a major issue with adding it to the report. [edited]
2025-09-10 22:04 danicuki: > <@davxy:matrix.org> Hmm interesting... indeed we could include the result of our last block import in the final fuzz report - it might serve as a helpful hint.  
> 
> However, should be noted that the evaluation order of some expressions is arbitrary, so with a block with the potential to trigger multiple errors the result could is a bit ambiguous.  
> It thus also expose some implementation details, which might not be super ideal.
> 
> I'm not against including it, just unsure.  
> Perhaps we should add it if it meaningfully speeds up the analysis.  
> What do you think, [@dave:parity.io](https://matrix.to/#/@dave:parity.io) 

You could list all potential errors in no particular order. Or at least reveal the first one. Because it is very very hard from our side to know why a particular block is rejected. When we have state value diff it is easier to debug and find reasons for the diff. But when next state doesnâ€™t change, we stay clueless and canâ€™t even challenge the fuzzer. 
2025-09-10 22:08 danicuki: Another idea: fuzzer target binaries are public. This means anyone can download other teams binaries and run over them the blocks and inspect the logs to find clues. For this reason, since you already run teams fuzzer target, you could share in the repo the running logs of all teams. [edited]
2025-09-11 11:44 davxy: As you are the main stakeholders, could I ask for one more review round before we merge?  
The changes are summarized in the PR description: https://github.com/davxy/jam-conformance/pull/47
2025-09-11 13:08 r2rtnl: https://github.com/davxy/jam-conformance/discussions/74
2025-09-11 16:02 charliewinston14: Is it correct that the ancestry set is maintained separately for each fork?
2025-09-11 16:15 davxy: The ancestry is determined using the P() function (see 5.3), which traces the chain backward starting from a specified header. 
For a block within one fork, the state in other forks is irrelevant. [edited]
2025-09-11 16:17 davxy: For the sake of fuzzing you can maintain a simple bounded queue... (as we never extend mutations chain) [edited]
2025-09-11 22:15 r2rtnl: https://github.com/davxy/jam-conformance/discussions/75
2025-09-11 23:55 sourabhniyogi: It is likely very useful to have a v1 .bin/.json ancestors (and maybe forks) trace for us to test our v1 implementations as we post them in the next 24-72 hours?  In the examples/v1, with the updated names  [edited]
2025-09-12 17:04 davxy: [V1 Enacted](https://github.com/davxy/jam-conformance/pull/47)
2025-09-12 17:38 sourabhniyogi: Can you suggest the proper home for this issue https://github.com/davxy/jam-conformance/issues/76 
2025-09-12 17:44 clearloop: I assume this should be under `w3f/jam-test-vectors` or https://github.com/polkadot-fellows/JIPs ? [edited]
2025-09-12 17:46 clearloop: pretty like a standard interface for `PVM-FFI`, I think Jan Bujak may have some inputs? according to the benchmark tool in polkavm, there is sort of arch of Module, compile, various steps, etc. [edited]
2025-09-12 17:48 clearloop: https://github.com/paritytech/polkavm/blob/master/tools/benchtool/src/backend.rs#L14 this is pretty like a universal interface, but I think it's still missing the host calls part for making it PVM in JAM focused [edited]
2025-09-12 18:06 sourabhniyogi: Jan Bujak: Can you advise?
2025-09-12 18:56 prematurata: ðŸ¥³ should we signal support of our targets somehow? in the ghissue maybe?  [edited]
2025-09-13 05:16 jan: Something similar to this but cut down:

https://docs.rs/polkavm/latest/polkavm/struct.RawInstance.html

Basically:
1. A function to turn a raw program blob into a module (i.e. a compilation step for recompilers; for interpreters this will be mostly a nop)
2. A function to instantiate a new VM instance from that module.
3. Then various getters/setters to modify that instantiated VM's state and be able to call into it.

We definitely do not want to have callbacks in the interface. There should be just a single `run` method that you call on the instance, and if it happens to trigger a host call then it should return a status code signifying that a hostcall was called. (Similar to how inner PVM invocation hostcalls work.) Same with handling page-faults. The various "read_bytes_8" functions are completely unnecessary; there should only be one function for reading bytes with a given length, and then using that to e.g. read a 32-bit number is just a few trivial bitshifts and bitors away. And of course, most of the stuff in your "Debug and Tracing" section should not be in a standard API at all.
2025-09-13 12:12 sourabhniyogi: OK!  I will adjust our pvm.h to match that API 100% and shift the callback process to the run method approach, thank you!
2025-09-13 18:21 danicuki: Do we need to maintain fuzzer target accepting multiple versions? Or should we keep only the latest (in this case, now, v1)?
2025-09-13 19:52 davxy: for what concerns our fuzzer, you can switch to v1 only. All new fuzzing sessions will use v1
2025-09-14 15:23 danicuki: Are you planning to add some examples that use ancestors on v1? As far as I saw, all 30 examples don't use ancestors feature. 
2025-09-14 17:06 davxy: Yes, but I've not implemented the feature yet. Indeed during the handshake the flag is turned off in our PeerInfo message
2025-09-14 17:06 davxy: new batch https://github.com/davxy/jam-conformance/pull/78 [edited]
2025-09-14 20:13 prematurata: https://github.com/davxy/jam-conformance/discussions/79
2025-09-15 02:42 clearloop: interesting, each time new traces come out, I can trigger the sum of failures from all other teams
2025-09-15 22:32 r2rtnl: davxy:  a question regarding ancestor support and sharing the same implementation of the (11.35) check between a fuzzer target and jam-test-vectors.

In GP 0.7.0, (11.35) requires two things:

1. The lookup\_anchor hash must be present in the ancestor set.
2. Its slot must match lookup\_anchor\_slot.

The issue Iâ€™m seeing is that the reports test vectors only provide recent\_blocks, which donâ€™t include slot information.

This creates a mismatch:

- To pass the reports test vectors, the implementation must skip the slot check and check only against recent\_blocks.
- For full ancestor support in fuzzer target, the slot check must remain.

Would it be feasible to adjust the reports test vectors to include slot information, or otherwise make it possible to construct the ancestor set so that the full (11.35) check can run? [edited]
2025-09-16 02:55 ascriv: https://github.com/davxy/jam-conformance/discussions/84
2025-09-16 09:07 davxy: Please try to run your implementation against minifuzz first

https://github.com/davxy/jam-conformance/pull/85

  â†³ 2025-09-16 10:01 r2rtnl: davxy: Does minifuzz assume that an implementation supports forks? Messages 5 and 7 seem to import a block with the same slot (3).
  â†³ 2025-09-16 10:39 davxy: In the next hours I'll share a set with forks and another without forks.
2025-09-16 09:08 davxy: May be a bit buggy :-) feel free to open a PR to improve it
2025-09-16 09:24 davxy: Teams that are already enrolled are encouraged to run it anyway, as I have found some minor issues in the targets (e.g., incorrect error message length, wrong argument order in PeerInfo, etc.).  
I will not fuzz your target if you have not successfully completed the first 20 steps in the examples/v1 folder using minifuzz [edited]
2025-09-16 10:21 tomusdrw: Is there any more details on the examples (i.e. expected state, etc)? We are getting a wrong state root at some point, but not really sure how I could debug it.
2025-09-16 10:38 davxy: I added a readme file to the examples folder. However, I'll copy the first 20 steps to the minifuzz folder to avoid midunderstandings
2025-09-16 10:46 clearloop: we have unit tests for `examples/v1`, e.g. decode/encode `*.bin` and see if things get matched, not sure if this is easier to test the format, will try the mini-fuzz script anyway
2025-09-16 10:56 tomusdrw: Ah, okay, perfect! I missed that.
2025-09-16 13:01 jaymansfield: davxy: Just a heads up, minifuzz seems to work until the invalid state root in step 29. It then shuts down after seeing the mismatch rather then proceeding with the final get state request.
2025-09-16 13:02 davxy: Yes this is known. See the README in the examples folder.
I'll prepare some other traces for minifuzz testing (with and without forks)
2025-09-16 13:04 davxy: In the examples we intentionally return a wrong root to emulate a bad target, and thus include the GetState message in the examples sequence
2025-09-16 13:04 davxy: But this is causing some confusion (you are the third to ask)
2025-09-16 13:04 davxy: so I'll provide a dedicated folder for the "self test"
2025-09-16 13:07 jaymansfield: This would help. I think the readme is clear in that it should fail, but not that minifuzz won't actually perform the get state itself after. That was the part missing. [edited]
2025-09-16 13:10 clearloop: wait, I just realized that `minifuzzer` stopped after 10 pairs on my machine without any error messages, if this is not normal as well

```
==========================================================================
Processing pair 10: 00000009_fuzzer_import_block.bin -> 00000009_target_state_root.bin
TX: import_block
RX: state_root

Stopping after 10 file pairs as requested
``` [edited]
2025-09-16 13:11 jaymansfield: Try adding the argument: --stop-after 30
2025-09-16 13:47 davxy: https://github.com/davxy/jam-conformance/tree/main/fuzz-proto#preliminary-self-testing
2025-09-16 13:47 davxy: I hope is clear now
2025-09-16 14:12 danicuki: I get error when try to run minifuzz:
```
$ python minifuzz/minifuzz.py -d examples/v1/forks --target-sock /tmp/jam_target.sock
Traceback (most recent call last):
  File "/Users/danicuki/dev/jam-conformance/fuzz-proto/minifuzz/minifuzz.py", line 9, in <module>
    from jam_types.fuzzer import FuzzerMessage
ModuleNotFoundError: No module named 'jam_types'
```

Where can I find the `jam_types` code? 
2025-09-16 14:14 clearloop: ```
pip install git+https://github.com/davxy/jam-types-py.git
```
2025-09-16 14:22 clearloop: davxy: I think there is a problem in the fork tests at `00000004` which propose `block.slot=1` with parent hash of the block in `00000003` (`block.slot=1` imported successfully) and expects import success, if I'm not mistaken, if this is a fork, it should have the parent hash of `block.slot=0`
2025-09-16 14:59 jaymansfield: I'm seeing the parent of 00000004 to be the one from the initialize in 00000001
2025-09-16 15:40 danicuki: minifuzzer should not compare message size, since error messages can differ from implementation to implementation, no?

```
Processing pair 7: 00000006_fuzzer_import_block.bin -> 00000006_target_error.bin
TX: import_block
Error decoding target response: Decoding <String> - No more bytes available (needed: 69 / total: 68)
Connection closed
```

I believe `FuzzerMessage(data=scale_bytes).decode()` might have a bug [edited]
2025-09-16 15:43 clearloop: can you pass that, from my logs `0x7003387fc23f422113a94dd5abc0b4fa31eae0108b4ac9ea6a133ffd14e315fb` is the parent `00000004` want

```
DEBUG  read: message(length): Info
DEBUG write: message(21): Info
DEBUG  read: message(length): Initialize(len=21)
DEBUG write: message(33): StateRoot(0xe174c1ea94e21958db28c9ea621f09ae2b360a9ecb6047afdbf94827d216e1db)

DEBUG  read: message(length): ImportBlock(slot=1, hash=0x252ff7a523a698b91e872f90f3f046fac90b80869bf9c7cf2000d627f4e7df3e)
DEBUG import: importing block(1)=0x252ff7a523a698b91e872f90f3f046fac90b80869bf9c7cf2000d627f4e7df3e, best block: 0
 WARN failed to import block: BadCoreIndex
DEBUG write: message(14): Error(BadCoreIndex)

DEBUG  read: message(length): ImportBlock(slot=1, hash=0x7003387fc23f422113a94dd5abc0b4fa31eae0108b4ac9ea6a133ffd14e315fb)
DEBUG import: importing block(1)=0x7003387fc23f422113a94dd5abc0b4fa31eae0108b4ac9ea6a133ffd14e315fb, best block: 0
DEBUG write: message(33): StateRoot(0x25b1bd54e8d0d5c82202ae51de2d378b8dfa13f637fe75bda672484c25e1c6a6)

DEBUG  read: message(length): ImportBlock(slot=1, hash=0xe63b5fa906678415f5b15ae6514d44b35e4dafa497f52a1640cbf374811b9c89)
DEBUG import: importing block(1)=0xe63b5fa906678415f5b15ae6514d44b35e4dafa497f52a1640cbf374811b9c89, best block: 1
 WARN import: Fallback state to 0
 WARN failed to import block: Parent mismatch, expected: 0x7003387fc23f422113a94dd5abc0b4fa31eae0108b4ac9ea6a133ffd14e315fb, got: 0x2bf11dc5e1c7b9bbaafc2c8533017abc12daeb0baf22c92509ad50f7875e5716
``` [edited]
2025-09-16 16:39 davxy: > I think there is a problem in the fork tests at 00000004

I don't see any issue

> I'm seeing the parent of 00000004 to be the one from the initialize in 00000001

Correct

Given the filename number associated to each block, the chain graph of the blocks (attempted to be imported) up to 7 should be:

```
1--+--2
   +--3
   +--4--+--5
         +--6--7
``` [edited]
  â†³ 2025-09-17 07:09 clearloop: thanks! I found that our impl has sort of bugs on falling back
2025-09-16 16:43 davxy: From the replies, it appears that:

- Blocks **2** and **5** fail to be imported.  
- Blocks **3** and **4** are successfully imported (actual fork).  

According to the fuzzer protocol, the chain is extended using the **last successfully imported block**.  

In other words, you will **never** see blocks 3 and 4 successfully imported and then the chain extended from block 3.

2025-09-16 16:43 davxy: https://github.com/davxy/jam-conformance/tree/main/fuzz-proto#example-session
2025-09-16 20:42 sourabhniyogi: After teams get through  "minifuzz" this week and a round or two of ancestry + forks this week and next, can we wrap up 0.7.0 and move collectively onto 0.7.1, say, on week of Sept 29?

Since there isn't _that_ much happening on 0.7.2, perhaps we can jump from 0.7.0 to [0.7.2](https://github.com/gavofyork/graypaper/releases/tag/v0.7.2)? [edited]
2025-09-17 03:36 ascriv: Should L be 24 for the tiny config? Cant pass 1757862468 otherwise 
2025-09-17 04:47 prematurata: > <@ascriv:matrix.org> Should L be 24 for the tiny config? Cant pass 1757862468 otherwise 

Yes
2025-09-17 04:48 prematurata: Anyone passing https://github.com/davxy/jam-conformance/discussions/79 can shred some light on this?
2025-09-17 05:45 danicuki: > <@ascriv:matrix.org> Should L be 24 for the tiny config? Cant pass 1757862468 otherwise 

Yes
2025-09-17 08:26 0xjunha: https://github.com/davxy/jam-conformance/discussions/91
2025-09-17 10:49 davxy: 
2025-09-17 11:38 clearloop: anybody knows what's the case of faulty\_000000029 ? from `000000030`, I found that the trace expects block `0xb13b648e9030118a6bf912aaca95a78b66c86cbd41d112b21393d4b896eaf864` in the history via getting the state of `0x91fcda538898b174da9b61af42c141fb0e1549e4e0dfc1ca8caec4c6185eeea5` (and this is the last block get imported exactly in my target) while the block `0xbi3b6...` does not exists at all in my importing process [edited]
2025-09-17 13:01 vinsystems: > <@clearloop:matrix.org> anybody knows what's the case of faulty\_000000029 ? from `000000030`, I found that the trace expects block `0xb13b648e9030118a6bf912aaca95a78b66c86cbd41d112b21393d4b896eaf864` in the history via getting the state of `0x91fcda538898b174da9b61af42c141fb0e1549e4e0dfc1ca8caec4c6185eeea5` (and this is the last block get imported exactly in my target) while the block `0xbi3b6...` does not exists at all in my importing process

https://github.com/davxy/jam-conformance/tree/main/fuzz-proto/examples/v1#warning-faulty-session
  â†³ 2025-09-17 13:02 clearloop: thank you so much! I have just spent hours on this ðŸ¤¦â€â™‚ï¸ [edited]
2025-09-17 14:58 davxy: https://github.com/gavofyork/graypaper/pull/497
2025-09-18 06:46 dakkk:  It seems I'm having a similar issue with that trace
2025-09-18 13:40 danicuki: > <@sourabhniyogi:matrix.org> After teams get through  "minifuzz" this week and a round or two of ancestry + forks this week and next, can we wrap up 0.7.0 and move collectively onto 0.7.1, say, on week of Sept 29?
> 
> Since there isn't _that_ much happening on 0.7.2, perhaps we can jump from 0.7.0 to [0.7.2](https://github.com/gavofyork/graypaper/releases/tag/v0.7.2)?

I agree we should go directly to 0.7.2
2025-09-18 14:12 clearloop: interesting, I just found that in the tracing tests (conformance reports/traces), there are programs have same code\_hash but different initial registers, ain't the code\_hash the hash of (metadata + standard program) that the initial registers should be fixed? [edited]
2025-09-18 16:54 boymaas: When running the minifuzz, do other teams also verify whether the author_index is determined using:  https://graypaper.fluffylabs.dev/#/38c4e62/0e1c040e3104?v=0.7.0 in key mode, as part of their header validation. I observe a delta in my implementation **only during the first epoch** of the jamtestvectors traces and the minifuzzer protocol test.

2025-09-18 19:13 sourabhniyogi: Poll: December 2025 In-person JAM Meetup @ PBA Lisbon
 https://github.com/davxy/jam-conformance/discussions/93
It would be fabulous if we had as many of the JAM implementer enrolled teams there as we possibly can!
2025-09-18 19:16 emielsebastiaan: Does anyone have specific dates?
2025-09-18 19:49 sourabhniyogi: Hello nikos -- we heard about the PBA JAM course way back in the JAM XP  May meetup -- it would be great to have JAM implementers assemble there.  Can you tell us about details so we can plan with you a bit?
2025-09-18 19:50 wirednkod: sourabhniyogi: the dates are not final
2025-09-18 19:50 wirednkod: we are recovering from PBA Bali and expecting some updates concerning the toaster - and then decide the final dates
2025-09-18 19:51 wirednkod: I would suggest to avoid polls with specific dates as the one above - but i will keep the JAM community up to date as soon as we have the final dates
2025-09-18 19:51 sourabhniyogi: Are you ok with 20-30 of JAM implementer headcount there?
2025-09-18 19:52 wirednkod: i cannot provide a definite answer to that at the moment as it may sound like a promise
2025-09-18 19:53 wirednkod: again - i would suggest for a bit of patience - im sure by the end of Sept we will have many more details to move forwrad
2025-09-18 19:53 wirednkod: but at the moment neither the date nor the participant numbers or profile are 100% defined
2025-09-18 19:53 sourabhniyogi: Absolutely, will be patient =)
2025-09-18 19:54 wirednkod: thank you
2025-09-19 16:37 shimonchick: Is there something with the JIP-5 altnames? Its a very basic algorithm, but applying it yields different values for me. Can somebody confirm if the current JIP-5 altnames in https://docs.jamcha.in/basics/dev-accounts are correct?
2025-09-19 18:09 sourabhniyogi: Hats off to Tianyi | SpaceJam ... with the FIRST JAM Implementation to match/beat Polkajam in performance!   
2025-09-19 18:10 sourabhniyogi: As teams develop PVM recompilers, should we aim to have two entries in the dashboard like Polkajam or just one?
2025-09-19 20:30 davxy: First of all, congrats to Spacejam for achieving such impressive results!  

That said, we've now entered the sub-ms domain, where my workstation setup isn't  deterministic at all. 
One run can be faster, and the next might not be.  

To avoid encouraging pointless sub-ms races, I think it makes sense to round measurements to millisecond granularity 
  â†³ 2025-09-19 21:48 clearloop: 0.05ms, it's driving me crazy ðŸ«  agree with not overkilling on this, from my side, I'm feeling exhausted indeed in the recent weeks, however I think the sub-ms in single dataset is still useful for checking if there are spaces to optimize XD for example, via seeing polkajam is faster in safrole/fallback, I know things can be done better, and I just found sth new can be optimized in our implementation, but I actually hope that I can't find since this stuff looks like endless XD [edited]
  â†³ 2025-09-20 03:33 jan: Nice! I'm always happy to see other implementations matching PolkaVM in performance. Congratulations to SpaceJam!
  â†³ 2025-09-20 03:34 jan: The next frontier will be recompilation speed (along with the new gas cost model which will be a lot more challenging to make fast), which isn't currently very well tested by these performance benchmarks.
2025-09-19 20:37 davxy: For example, one could gain some *pointless* advantage just by removing **all** the logs.  
I've seen some implementations keep a few logs, while others completely removed them.  
We even spotted implementations removing signature checking ;-) (quickly restored, of course).  

Honestly, we don't care much about these micro-optimizations.  
What really matters is avoiding spending orders of magnitude more time fuzzing a target compared to the baseline.
2025-09-19 22:01 sourabhniyogi: Were the (now missing) logs useful to you?

I believe shifting the focus from micro-optimizations to recompilation is important.  We've been conditioned to care about the small (\<50%) differences because of

```
(M3) HALF-SPEED: Conformance and 50% of required performance (including PVM impl): 100,000 DOT + 1,000 KSM
(M4) FULL-SPEED: Conformance and 100% of required performance (including PVM impl): 100,000 DOT + 1,000 KSM
```

Its pretty obvious that performance is all about recompilation here, and I think its valuable to get more teams focussed on this.

We know its not an thing for M1, but since we know with 100% certainty its a not-pointless goal for M3/M4, its entertainment now but useful entertainment while we're waiting for like, 1.0 ratification.

Why not introduce at least 1M-10MM million gas of randomly generated \[but still valid\] PVM byte code interpretation to always consume > 10ms (for baseline polkajam) so we can improve performance on at least _some_ traces?

By introducing this a > 10x factor, 50% differences become visible in `import_max`, and:

1. we have a baby step towards PVM fuzzing
2. recompiler teams can play a useful substantive M3 / M4 optimization game amidst M1 fuzzing.
3. more teams will get engaged on recompiler (because we love optimizing)

PVM fuzzing is valuable to attack, I presume, right within M1, is it not? [edited]
2025-09-19 22:56 clearloop: my testing linux machine is likely 10x-20x slower than the fuzz machine, can confirm that some of our big optimizations were actually started from 1ms on my machine which could be just 0.1ms/0.05ms in the testing machine, however at the current level, for spacejam we won't optimize more
â€¨out of the performance part, I think we're feeling anxious mostly caused by there is no obvious threshold for the stable implementation, the performance of polkajam is really hard to chase ( we started from 20x~50x slower, and some parts of polkajam are still unbelievable/insane to us atm even we are in the same language ) and sometimes I'm feeling like I'm working like a slave to chase it ðŸ« 
â€¨hope the threshold of M3/M4 or the baseline for the current M1 will not even in X ms but based on X0 ms or X00ms, which is for sth like: ã€Œthere are 6s in the block, if you can import most of blocks within 30ms avg on machine A, it's totally enough for the stability of our network, you have done a great job, I won't force you to archive 0.X ms since all we know there are indeed differences in implsã€ [edited]
2025-09-20 03:38 jan: > hope the threshold of M3/M4 or the baseline for the current M1 will not even in X ms but based on X0 ms or X00ms

This is not official, but I imagine for M3/M4 we'll look at what's achievable (by looking the fastest implementation), and just pick a threshold that's slightly lower than that so that it won't be limited to only the most performance-oriented teams and/or languages (where "slightly lower" is very much TBD; we'll probably know how much that is exactly once we can run more real-world tests on an actual JAM chain)
2025-09-20 03:40 jan: I can see a world where some parts of the protocol will be a bottleneck and that could make it less important to optimize other parts because they wouldn't actually make things faster in practice. [edited]
2025-09-20 03:41 jan: But, again, this is still TBD until we can do more real-world testing instead of microbenchmarks.
2025-09-20 03:43 jan: > Why not introduce at least 1M-10MM million gas of randomly generated [but still valid] PVM byte code interpretation to always consume > 10ms (for baseline polkajam) so we can improve performance on at least some traces?

For purely PVM drag racing it probably doesn't make sense to use a general-purpose fuzzer/harness like this and it'd be a better idea to make a dedicated one which *only* does PVM (like I'm planning to have for M3/M4)
2025-09-20 03:45 sourabhniyogi: Life is what happens when you're busy making plans =)
2025-09-20 04:49 ascriv: How will the amount of M1 conformance vectors be defined? Based on amount of vectors polkajam-int can run in 3 days (e.g.) at the time of conformance? Or something else?
2025-09-20 11:42 rustybot: https://github.com/davxy/jam-conformance/pull/95
2025-09-20 12:07 boymaas: Could something have gone wrong with the last performance run? When I bench in a VM, I already get much lower values (between 5x and 10x lower) using the same weighted scoring algorithm.
2025-09-20 12:20 davxy: Indeed there is a big regression for jamzig. I'll try to re-run, but I used exactly the same setup for all the targets.
  â†³ 2025-09-20 12:25 clearloop: could you please try to download the latest spacejam binaries as well, I had an update of our crypto stuffs late yesterday, possibly provides 25% optimization of our basic fallback mechanism, not sure if it really works
2025-09-20 12:20 clearloop: looks like this new rule has a lot of effects in crypto related stuffs XD
2025-09-20 12:22 boymaas: Thanks, davxy. An hour ago, I released a new binary that also addresses the last failing reports.
2025-09-20 12:34 davxy: Perhaps my message was not clear:
- I'm not sustaining this kind of competition 
- I'm not here to run your impl on each update (for a sub ms race)
- I may run impls with strange behavior

I will run your update on next round [edited]
2025-09-20 12:35 davxy: FWIW fuzzer protocol is public, traces are public, anyone can check their perfs
2025-09-20 12:49 boymaas: No problem. It's just an unusual regression I do not understand. It will resolve itself the next run. There's no rush.
2025-09-20 12:53 ascriv: I think I speak for everyone when I say we all really appreciate the work youâ€™re basically soloing @davxy 
2025-09-20 12:58 clearloop: understood, sort of devops related stuffs for all teams are indeed need mental health insurance ))
2025-09-20 13:02 davxy: Well the point is that it is **literally** not my job to bench your impls. I work on an implementation and I built a fuzzer. That so far is the most effective [edited]
2025-09-20 13:03 davxy: Well your impl has a very strange regression, so I'll check that of course :-)
2025-09-20 13:05 boymaas: Thank you; Much appreciated.
2025-09-20 14:46 ycc3741: 
2025-09-20 14:47 ycc3741: davxy: 

When testing [trace/preimage_light/0000008](https://github.com/davxy/jam-test-vectors/blob/master/traces/preimages_light/00000008.json), we found some issues with [pi_V[v]_g](https://graypaper.fluffylabs.dev/#/38c4e62/19ea0119fc01?v=0.7.0), as follows:

We know that in tiny mode, the parameters are R = 4, E = 12, which means the cores where the guarantors for slot 7 and slot 8 reside will rotate.

We observed that originally our pi_V[v]_g was 2. In slot 8, we received guarantee_extrinsic with two elements belonging respectively to slot 7 and slot 8. According to the core assignment for slot 8 â€” [0,1,1,0,0,1] â€” and for slot 7 â€” [1,0,0,1,1,0], we confirmed that the reports received for slot 7 and slot 8 were both valid.

Thus, we incremented pi_V[v]_g twice by +1 [(since kappa_v_prime is G according to GP 0.7.0 formula 13.5)](https://graypaper.fluffylabs.dev/#/38c4e62/19f40119f401?v=0.7.0), resulting in pi_V[v]_g_prime = 4.

However, according to GP 0.7.0 formula 13.5 (kappa_v_prime is G), it only checks whether kappa_v_prime is in G. But doesnâ€™t this conflict with the description above 13.3:

[g: The number of reports guaranteed by the validator.](https://graypaper.fluffylabs.dev/#/38c4e62/197700197a00?v=0.7.0)

Shouldnâ€™t both reports be counted? Or should we simply follow the [formula (13.5)](https://graypaper.fluffylabs.dev/#/38c4e62/19f40119f401?v=0.7.0) and add +1 just once?
2025-09-20 14:47 ycc3741: post.png
2025-09-22 12:24 dave: IIRC Gav said the GP behaviour in this case is intentional, to keep things simple. I agree the counting behaviour you describe would probably be more useful. You can propose a GP change by making a PR in the GP repo, if the change is simple I think there's a fair chance of it being accepted
2025-09-23 01:29 ycc3741: THX a lot
2025-09-23 01:29 ycc3741: we will deal with it later
2025-09-23 16:18 davxy: https://github.com/davxy/jam-conformance/discussions/98#discussioncomment-14488639
2025-09-25 15:58 danicuki: Many of the new fuzzer batch are related to on_transfer, which was dropped on GP 0.7.2. 
Do you think it is worth at this point to have tests that use on_transfer?
2025-09-25 16:33 prematurata: in my case i found other issues not directly related to ontransfer so it was worth it I'd say
2025-09-25 18:43 davxy: Keep in mind that it is not mandatory to pass every case. You may decide that some are not worth addressing and leave them as is.  
This is the final batch for 0.7.0; after this, we will move on to 0.7.1.
2025-09-30 07:45 danicuki: > <@davxy:matrix.org> Keep in mind that it is not mandatory to pass every case. You may decide that some are not worth addressing and leave them as is.  
> This is the final batch for 0.7.0; after this, we will move on to 0.7.1.

Wouldnt worth to move directly to 0.7.2? We at Jamixir already have 0.7.2 ready to be fuzzed. 
2025-10-01 14:58 jaymansfield: I've published some very basic 0.7.1 safrole vectors if anyone is interested: https://github.com/javajamio/javajam-trace
2025-10-01 17:15 davxy: 0.7.1 vectors and fuzzer are nearly done. Code under review :-)
2025-10-02 19:25 shimonchick: On JAMSNP CE131/132 what does the attempt bit mean? Does it signalize which of the 2 protocols to apply? https://docs.jamcha.in/knowledge/advanced/simple-networking/spec#ce-131132-safrole-ticket-distribution
2025-10-02 20:12 dave: Each validator generates 2 tickets per epoch, attempt indicates which one this is [edited]
2025-10-03 17:36 davxy: 0.7.1 test vectors RC

https://github.com/davxy/jam-test-vectors/pull/96
2025-10-03 17:38 davxy: If you have a 0.7.1 target available, just drop a message in your jam-conformance issue
2025-10-08 20:16 emielsebastiaan: Can anyone confirm, please? 

https://github.com/davxy/jam-test-vectors/issues/100
2025-10-09 04:01 tvvkk7: We also found the same issue. 
Service 0 miss a storage item.
2025-10-09 12:01 danicuki: > <@tvvkk7:matrix.org> We also found the same issue. 
> Service 0 miss a storage item.

We didn't find this issue on our side. Could you point on GP formulas what we are eventually missing?
2025-10-09 12:02 prematurata: i dont see that problem either but i do have this https://github.com/davxy/jam-test-vectors/issues/101
2025-10-09 12:15 tvvkk7: The write host call will append a new storage item. 
You can see the service items changes from 2 to 3.
2025-10-09 13:03 danicuki: The only write to storage is this on our side:
```
0x7472616e73666572726564 => 0x0200000064000000000000006769667400000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
```

and as far as I saw, our final services storages are identical to those on the vectors. 
2025-10-09 13:20 tvvkk7: Yes, we write a storage, but the post-state storage is empty in the [test vector](https://github.com/davxy/jam-test-vectors/blob/372204dfba44c5652fbccd974d2da596a2352205/stf/accumulate/tiny/transfer_for_ejected_service-1.json#L376)
2025-10-09 16:45 davxy: https://github.com/davxy/jam-test-vectors/pull/102
2025-10-11 10:19 davxy: It seems there has been a regression affecting the alignment of our implementations after the 0.7.1 release.
While the implementations remain aligned for basic public test vectors, issues appear as soon as I instantiate our *fuzzy* service.

To encourage independent debugging and avoid excessive back-and-forth, Iâ€™ve published a trace set that instantiates and executes this service in the simplest form (i.e. it doesn't do anything particular).
Please run it on your side and report any issues in the PR

https://github.com/davxy/jam-test-vectors/pull/103 [edited]
  â†³ 2025-10-11 10:44 dakkk: it seems we have a discrepancy on services' balances in many tests; I will investigate further [35 failed, 165 passed] [edited]
2025-10-14 17:54 aang114: image.jpg
2025-10-14 17:55 aang114: Hi, does anyone know if the state component Î¸ is supposed to be ordered by the service index? Because one of our failing fuzzer tests seems to be because Î¸ is not ordered by service index (i.e https://github.com/davxy/jam-conformance/blob/main/fuzz-reports/0.7.0/reports/gossamer/1757406598/report.json#L73-L74). Thanks
2025-10-14 18:09 jaymansfield: Replied in "Let's JAM".
2025-10-15 18:42 clearloop: anybody can share the trace of `accumulate::test_accumulate_ready_queued_reports_1` ?

```
    DEBUG accumulate: host call: 1
    DEBUG accumulate: fetch kind: 0
    DEBUG accumulate: host call: 1
    DEBUG accumulate: fetch kind: 0
     INFO service: This is Accumulate in the Test Service 6c1h with 9 items
    DEBUG accumulate: host call: 1
    DEBUG accumulate: fetch kind: 14
    DEBUG accumulate: host call: 1
    DEBUG accumulate: fetch kind: 14
    TRACE accumulate: PVM execution continued for service 1729, reason: Halt
    DEBUG accumulate: Service 1729 used 2980 gas during accumulation
    test accumulate::test_accumulate_ready_queued_reports_1 ... FAILED

---

gas mismatched, expected 2980, got 16240
```

I found that I can pass the basic type check of accumulate items (transfers + operands) but it looks like my operands got no-op, have checked that my operands match the definition in `0.7.1` so currently no ideas what to debug now XD [edited]
  â†³ 2025-10-16 11:24 clearloop: solved, encoding problem indeed
2025-10-16 18:33 aang114: Hi guys, one of the failing fuzzer traces of ours is **1758622000**, where the only difference is the difference in Ï‡V (i.e in the delegator service index).

But based on my understanding, in GP 0.7.0, Ï‡V cannot be updated unless the "designate" host-call is called.

However, it does not seem that the "designate" host-call is being called for the trace **1758622000**. Since I noticed that others implementors had the exact same difference, **could i confirm if anyone was able to solve this trace?** Thanks

CC: ascriv | Jamzilla  Junha | FastRoll [edited]
2025-10-16 18:36 aang114: image.png
2025-10-16 18:36 aang114: As you can see above, only the v\* service index can update Ï‡V (by updating v'). And if v\* is not the manager service index, it can only update Ï‡V if the "designate" host-call is called. [edited]
2025-10-16 19:19 davxy: [@danicuki:matrix.org](https://matrix.to/#/@danicuki:matrix.org) 

Minor state diffs - mostly the kind of issues that you and the other teams have already faced in previous iterations.  
Before the 0.7.0 fuzzing campaign, the diffs were mostly about simple GP misinterpretations. Now the nodes are getting more robust, and most discrepancies stem from features introduced in 0.7.1 or from hostcalls we hadn't tested before excercising less trivial behaviors.  
The "fuzzy" service now tests almost all hostcalls.  
We'll submit some reports for 0.7.1 in the coming days, by the way.  
Have you already checked the fuzzy trace set?

https://github.com/davxy/jam-test-vectors/tree/master/traces/fuzzy
2025-10-16 19:34 davxy: These particular "fuzzy" traces will be renamed to "fuzzy-light".  
Another "fuzzy" set will be added to cover a broader range of hostcalls.
2025-10-16 20:53 sourabhniyogi: davxy: For "based on my experiments there is still NO target I can fuzz continuously for minutes in a row. " can you advise what teams should do to replicate  "for minutes in a row" case and replicate your observations when it fails? 
2025-10-16 21:02 jaymansfield: > <@sourabhniyogi:matrix.org> davxy: For "based on my experiments there is still NO target I can fuzz continuously for minutes in a row. " can you advise what teams should do to replicate  "for minutes in a row" case and replicate your observations when it fails? 

He clarified right above your post. Looks like he was referring to fuzzing without encountering state diffs, and not actual running time based failures.
2025-10-17 08:03 ascriv: Bless is called, and that one modifies the designate service index. But Iâ€™m still getting the wrong output for now because even though the manager modifies the designate service, (12.17) says we should use the designate service returned by the accumulation of the managerâ€™s returned designate service. Might just leave this test since I think that might be a bug in 0.7.0 anyway  [edited]
2025-10-17 08:16 danicuki: our impl works with `fuzzy` set. There was a bug related to transfer amounts that we fixed recently based on these traces. Thanks
2025-10-17 15:39 aang114: 
2025-10-17 15:40 aang114: yes exactly. that's good that we are in agreement. i'll leave it too then. Thanks :)
2025-10-17 17:11 sourabhniyogi: Hi nikos-pba nikos Many of us are planning our holiday travels of Nov/Dec and I wanted to check one last time if there is anything happening on https://polkadot.academy/jam/ ?   
2025-10-17 17:29 wirednkod: Hey sourabhniyogi - thank you for the ping. I will have more details beginning of next week - after a meeting with Gav that we will understand at what stage the hardware is, that will define the academy days. Apologies for the delay - but we are still identifying all the parameters that will define the most suitable days for executing the JAM course
2025-10-21 11:06 clearloop: anybody met this in the fuzzy test set (`00000046`)?

```
    DEBUG accumulate: account info: ServiceInfo { version: 0, code: [253, 146, 241, 84, 20, 126, 246, 152, 123, 145, 212, 198, 252, 157, 249, 25, 128, 6, 224, 88, 135, 58, 167, 53, 219, 156, 200, 47, 95, 228, 85, 144], balance: 204657, accumulate: 10000, transfer: 10000, total: 147033, offset: 0, items: 2, creation: 4, update: 41, parent: 0 }
     WARN service: Panic handler called!
     WARN service: Panic message: panicked at services/fuzzy-service/src/instruction.rs:1011:9:
    zombify - unexpected items count 0 after cleanup!!!
```

we have `2` items which matches the expected result while the `zombify` service says we have 0 items, seems that this is not caused by encoding problems since I have checked the encoding order in both `0.7.1` and `0.7.2` for the `info` host call [edited]
2025-10-21 11:30 davxy: This may be a bug in the fuzzy service. As far as your state root matches the expected root you can ignore it.
I'll update the traces soon anyway
2025-10-21 16:20 sourabhniyogi: Is there a new fuzzy set somewhere already, or coming soon? 
2025-10-21 16:24 davxy: Very soon 
2025-10-26 16:23 aang114: Hi, could I please confirm that [1758819527/00000598.json](https://github.com/davxy/jam-conformance/blob/main/fuzz-reports/0.7.0/traces/1758819527/00000598.json) and [1758708840/00000958.json](https://github.com/davxy/jam-conformance/blob/main/fuzz-reports/0.7.0/traces/1758708840/00000958.json) are **invalid trace files** since the post-state state-root is `0x0000000000000000000000000000000000000000000000000000000000000000` and the post-state keyvals are empty? Thanks
  â†³ 2025-10-26 21:18 vinsystems: https://github.com/davxy/jam-conformance/discussions/99 
2025-10-26 18:15 hanayukii: Clarification: should vStar be assigned to the next PartialStateSet.Designate for subsequent runs of  Î”* ?
https://graypaper.fluffylabs.dev/#/38c4e62/17b60117b601?v=0.7.0
From our recent test results and discussion between the PVM and Accumulation implementations, we observed that in Î”* updating PartialStateSet.Designate with vStar seems necessary to make the Chi data correct in some cases, also for the following PVM work. This behavior appears reasonable by the opinion on PVM side. however, we couldnâ€™t find explicit statement in the Graypaper (both v0.7.0 or v0.7.2) indicating that PartialStateSet.Designate must be updated with vStar before the subsequent Î”â‚ calls in Î”*

Formula from 0.7.0 (12.17)
(mâ€², a, v, zâ€²) = Î”â‚(e, w, f, m)
vâ€² = (âˆ†1(e, w, f , vâˆ—)e)v

Weâ€™d like to confirm whether this update â€” assigning vStar to PartialStateSet.Designate â€” is intended by the spec or defined elsewhere in the Graypaper, or if we are misunderstanding the intended semantics. Thanks! 
2025-10-27 05:17 hanayukii: nvm, found previous source about this issue https://github.com/davxy/jam-conformance/discussions/98#discussioncomment-14488639
2025-10-27 10:55 davxy: 1. New traces for the fuzzy profile in **jam-test-vectors** (GP 0.7.1): [#104](https://github.com/davxy/jam-test-vectors/pull/104)  
2. New traces/reports in **jam-conformance** (GP 0.7.1): [#105](https://github.com/davxy/jam-conformance/pull/105)  

Please prioritize addressing **1** first, as it will likely resolve most of the reports listed in **2**.

2025-10-28 23:56 celadari: Hi, I have a doubt regarding a specific test vector `different-core-same-guarantors` https://github.com/davxy/jam-test-vectors/blob/master/stf/reports/tiny/different\_core\_same\_guarantors-1.json\`

we have two work reports with same `lookup_anchor` and different `lookup_anchor_slot`:

```json
"context": {
                        ....
                        "lookup_anchor": "0x16bda47e5a68daf53c39ddee8af4ecaced7e87f3f0ac9da5a6f4f9e41350d319",
                        "lookup_anchor_slot": 6,
                    },
```

and

```json
"context": {
                        ...
                        "lookup_anchor": "0x16bda47e5a68daf53c39ddee8af4ecaced7e87f3f0ac9da5a6f4f9e41350d319",
                        "lookup_anchor_slot": 10,
                    },
```

according to https://graypaper.fluffylabs.dev/#/1c979cb/15f00215f902?v=0.7.1 that would require us to have stored two headers with the same hash but different time slot => not possible

Is there something I'm missing ?

PS: I originally posted this message in the Let's JAM chat room but this chat may actually be a better fit for my question
2025-10-29 08:16 davxy: 
2025-10-29 09:01 davxy: Fixed
  â†³ 2025-10-29 12:07 celadari: Thanks :)
2025-10-31 09:32 greywolve: Could anyone share a PVM trace for t[his storage trace](https://github.com/davxy/jam-test-vectors/blob/master/traces/storage/00000014.json) ? Since v0.7.1 we seem to be using more gas than required, but we get the correct service post state. So it's been really tough to debug. (also failing 13, 17, 39 and 95, in that batch in a similar way) [edited]
2025-11-03 12:01 danicuki: GM. Are the binaries on `fuzz-proto/examples/v1/faulty/` updated to latest 0.7.1? They seem to still be on version 0.7.0 and not compatible other files
2025-11-04 18:34 davxy: Needs to be updated, I filed an issue to remember it :-)
2025-11-04 18:35 davxy: I've updated the REPORTS table.
First stop on the road to glory: passing the fuzzy traces in the jam-test-vectors repo...

https://github.com/davxy/jam-conformance/tree/main/fuzz-reports
2025-11-04 18:35 davxy: (jam-test-vectors traces are now included in the table as well)
2025-11-05 17:04 celadari: Hi,

Can I kindly request to use `preimages-blob` and `preimages-status` in testvectors `stf/preimages` like in testvectors `stf/accumulate` (instead of ~~`preimages`~~ and ~~`lookup-meta`~~)

in `stf/preimages`:
https://github.com/davxy/jam-test-vectors/blob/84213c3fa619f26d77f61601a7dd77048f6bbf7a/stf/preimages/preimages.asn#L28

in `stf/accumulate`:
https://github.com/davxy/jam-test-vectors/blob/84213c3fa619f26d77f61601a7dd77048f6bbf7a/stf/accumulate/accumulate.asn#L38

it would help us a lot with parsing to always keep the same between testvectors :)
2025-11-05 17:29 davxy: It's possible, but it should actually be done the other way around (which is more correct and closer to the GP): modify the preimages-status key in `std/preimages` to also store the requested pre-image length. If you agree, please open an issue in my jam-test-vectors fork so I can update it before 0.7.2 vectors are released. [edited]
  â†³ 2025-11-05 17:39 celadari: Sure !

I just made the ticket
https://github.com/davxy/jam-test-vectors/issues/107
2025-11-05 17:40 jaymansfield: Hey davxy, how close are we from 0.7.2 vectors?
2025-11-10 17:40 davxy: Introducing the changes for version 0.7.2 should be straightforward, so we might roll them out next week 
2025-11-10 17:42 davxy: Important stuff ...
[New test vectors](https://github.com/davxy/jam-conformance/pull/112) added to jam-conformance repo, this time targeting ed25519 crypto implementations.
These excercise very edge cases addressed by [ZIP-215](https://zips.z.cash/zip-0215), but that are quite easily exploitable by an attacker, so verifying compliance is important.
Currently identified compliant libraries: `ed25519-consensus` (Rust), `ed25519-zebra` (Rust), `ed25519consensus` (Go).
If you're not using one of these libraries is very probable that you will fail test vectors processing.
In that case open an issue in the repository indicating which library you are using (for tracking only, not for fixing :))).
If switching libraries is not an option, the changes should be straightforward in most cases, but needs changes in the library itself

2025-11-11 09:40 clearloop: davxy: If the zombify service integrates the info host call with the encoding order defined in https://graypaper.fluffylabs.dev/#/1c979cb/33920033ba00?v=0.7.1 ? all of our failing tests in fuzzy_light are related with the zombify service, I found it likely caused by the info call bcz once I hardcode an empty code hash to the result info, the panic disappeared, so either our encoding is incorrect or there is any condition based on code hash in the zombify service
  â†³ 2025-11-11 10:04 clearloop: okay, I found it is a bug in my impl, now the registers provide length [edited]
2025-11-15 18:07 sourabhniyogi: Its been a quiet week davxy -- can we get a forecast for the rest of the month/year?
2025-11-15 18:28 rustybot: - Yesterday I published one round for the existing vectors.
- I will very likely produce another 0.7.1 batch.
- (very short term) I would like to prepare the 0.7.2 release.
- (short term) We are coordinating the handover of fuzzing operations to a dedicated W3F team.
- (medium term) Another dedicated team should handle the prototyping of the M2 fuzzer. [edited]
2025-11-16 21:14 danicuki: Hi. I am getting "invalid extrinsic hash" validation on cases. 1761653121, 1761654464, 1761664166
Could someone help me sharing the "unhashed" extrinsic binary (Formula 5.5  - a = [ET (ET ),EP (EP ),g,EA(EA),ED (ED )] ? It is hard to debug without this raw binary.
? Thank in advance
  â†³ 2025-11-16 21:26 celadari: Did you check you have the right encoding (sometimes the order of some fields change subtlety between versions) ?

You can check for the .bin file to make sure you have the right encoding (except g...)
  â†³ 2025-11-17 07:24 danicuki: > <@celadari:matrix.org> Did you check you have the right encoding (sometimes the order of some fields change subtlety between versions) ?
> 
> You can check for the .bin file to make sure you have the right encoding (except g...)

I checked and seems to be correct on my side. Thatâ€™s why would help to have someone elseâ€™s encoded binary. 
  â†³ 2025-11-17 08:16 danicuki: Whatâ€™s strange is that all other blocks are ok, so seems to be something very specific to these 3 cases. 
  â†³ 2025-11-17 11:57 tomusdrw: ```
0x03170a2e7597b7b7e3d84c05391d139a62b157e78786d8c082f29dcf4c11131403170a2e7597b7b7e3d84c05391d139a62b157e78786d8c082f29dcf4c111314828f41a8fa6f27c857bac5b4821eadfe3ef588c6ba15abb105dea6eee3f10a07c9b19dee24aae44e05fc5fdfd8cacf49e1c153f5ef7f3224db91806d0b6fff46ab29e6dc16755d0071eba349ebda225d15e4f910cb474549c47e95cb85ecc4d6
```
That's for `jam-conformance/fuzz-reports/0.7.1/traces/1761653121/00000188.json` in our impl. Let me know if this helps or you need anything else.
  â†³ 2025-11-17 14:02 danicuki: Thanks for the help. 188 is ok here. I need 190, that's where the difference appears
  â†³ 2025-11-17 21:03 tomusdrw: `0x03170a2e7597b7b7e3d84c05391d139a62b157e78786d8c082f29dcf4c11131403170a2e7597b7b7e3d84c05391d139a62b157e78786d8c082f29dcf4c111314e4e03701c1ca531383c65ce6dc33c73ec1bf2f03a233c3c932b74dc970e647b3d7ee619e36bc3fdc58e59067edf50e3464143c7cfac83c1a226a1d374ee71c7fab29e6dc16755d0071eba349ebda225d15e4f910cb474549c47e95cb85ecc4d6` here you go - that's from 190
  â†³ 2025-11-18 08:58 danicuki: this helped a lot. I know now that my diff is in `g`. One last ask: could you provide the unhashed raw `g` binary ?
  â†³ 2025-11-18 10:53 tomusdrw: ```
0x019aeef530233f70494f3277d66de35f606040c4a5968525ffba283d1078830a1825000000030000952f23655eeb6275733c465cce2765c235f03fc962fe4bf2ca6e9afa86121eb39214aac263c25f4c23763c58231b1e8e9a85e0191e40d07463a5524953ad810a0400011ac2eae26820496f48ad13a4b9fe69cafb78760c4b53f3e296dea490486f300c111565d8655e0135d60ff122f4b04e4e6c065b264a25fde45a8a08b5796f070500bd9663694c8b3f841a106ba1c9165e9e8939dceb4f2546d64029ca98c50489719654432f5ad8a1e63ccff7f4345b26686190d8e486e505eb0b3edb3c8088270d
```
  â†³ 2025-11-18 10:53 tomusdrw: here it is
  â†³ 2025-11-18 14:53 danicuki: thank you so much. This helped me a lot and I found the bug! <3
  â†³ 2025-11-20 07:44 hey_im_stas: FYI â€” at FluffyLabs weâ€™re using our Codec Tool to compare blob diffs:
(https://codec.fluffylabs.dev/
)

It automatically detects the blob type, but you can also select it manually from the dropdown. After pasting the expected blob and the actual blob (or vice versa), the tool displays the decoded differences on the right-hand side.
  â†³ 2025-11-20 07:48 hey_im_stas: Maybe not necessarily useful in this specific case, but still worth mentioning.
  â†³ 2025-11-20 13:37 danicuki: Awesome. Thanks for the tip. 
2025-11-17 11:18 davxy: New batch for 0.7.1:

https://github.com/davxy/jam-conformance/pull/115
  â†³ 2025-11-17 11:46 clearloop: when will this getting merged? still have 1 report not passed in my local version
2025-11-17 11:19 davxy: I think this is the last one before jumping into 0.7.2
2025-11-17 11:24 davxy: For a couple implementations, it has become hard to trigger a diff or a crash without adding block mutations, which is a good sign 
2025-11-17 11:50 clearloop: curious if there is sorting for drawn validators, for trace `1761654684`, the program ends with an designate instruction which updates the drawn list however it doesn't match what expects in the test, just have one state diff in drawn validators

```
    DEBUG accumulate: designate-ed25519: validators=["ab0084d01534b31c1dd87c81645fd762482a90027754041ca1b56133d0466c06", "ad93247bd01307550ec7acd757ce6fb805fcf73db364063265b30a949e90d933", "f30aa5444688b3cab47697b37d5cac5707bb3289e986b19b17db437206931a8d", "4418fb8c85bb3985394a8c2756d3643457ce614546202a2f50b093d762499ace", "cab2b9ff25c2410fbe9b8a717abb298c716a03983c98ceb4def2087500b8e341", "8b8c5d436f92ecf605421e873a99ec528761eb52a88a2f9a057b3b3003e6f32a"]
---
    DEBUG polkajam-ed25519: ["ab0084d01534b31c1dd87c81645fd762482a90027754041ca1b56133d0466c06", "8b8c5d436f92ecf605421e873a99ec528761eb52a88a2f9a057b3b3003e6f32a", "4418fb8c85bb3985394a8c2756d3643457ce614546202a2f50b093d762499ace", "cab2b9ff25c2410fbe9b8a717abb298c716a03983c98ceb4def2087500b8e341", "ad93247bd01307550ec7acd757ce6fb805fcf73db364063265b30a949e90d933", "f30aa5444688b3cab47697b37d5cac5707bb3289e986b19b17db437206931a8d"]
    DEBUG spacejam-ed25519: ["ab0084d01534b31c1dd87c81645fd762482a90027754041ca1b56133d0466c06", "ad93247bd01307550ec7acd757ce6fb805fcf73db364063265b30a949e90d933", "f30aa5444688b3cab47697b37d5cac5707bb3289e986b19b17db437206931a8d", "4418fb8c85bb3985394a8c2756d3643457ce614546202a2f50b093d762499ace", "cab2b9ff25c2410fbe9b8a717abb298c716a03983c98ceb4def2087500b8e341", "8b8c5d436f92ecf605421e873a99ec528761eb52a88a2f9a057b3b3003e6f32a"]
``` [edited]
2025-11-17 13:13 prematurata: https://github.com/davxy/jam-conformance/discussions/116
2025-11-17 14:24 clearloop: https://github.com/davxy/jam-conformance/discussions/117
2025-11-18 10:53 tomusdrw: 
2025-11-19 14:00 danicuki: It's been a wild month, catching super hidden corner case bugs. Great work!
2025-11-21 15:58 emielsebastiaan: What are the realistic preconditions that still need to be met before W3F can accept M1 submissions, and what timeline should teams reasonably expect for that process? 
(Asking for a friend.)
2025-11-21 20:31 davxy: We are currently transferring knowledge about the fuzzing tools to the w3f auditors team. I expect this process to be completed within one month. After that, timelines and next steps will be up to w3f.

As one of the fuzzer devs, I can confirm that the minimum supported GP version is 0.7.1. Earlier versions lacked many features, and the engine was still buggy and immature, similar to your implementation two months ago. We did not backport all fixes and instead moved forward.

The fuzzer for 0.7.1 will be properly tagged.

Since we'll release a 0.7.2 node and fuzzer very soon (hopefuly next week), it may make more sense to target 0.7.2 because:
1. The changes from 0.7.1 to 0.7.2 are trivial.
2. 0.7.2 is the current GP version.
3. Our 0.7.1 already includes a few 0.7.2 changes (accidentally included and never reverted)

IMO, 0.7.2 is the right version to accept for the M1 official audit starting in early January.

Of course my word is not authoritative. I'm not the one handing out the prize! [edited]
2025-11-21 20:45 emielsebastiaan: Thank you, Davide. We want to express our appreciation for all the feedback weâ€™ve received from the fuzzer activities so far; we wouldnâ€™t be as far along without it.
2025-11-24 18:12 danicuki: Doubt about `info` host call. On 0.7.1, it uses w11 and w12 for f and l. On 0.7.2 it uses w9 and w10. Which one are you using on current conformance reports?
2025-11-24 20:09 davxy: Our implementation unintentionally included the changes formally introduced in version 0.7.2 from the very beginning. When this was noticed, I opened https://github.com/gavofyork/graypaper/pull/480 (also mentioned in 0.7.0 vectors https://github.com/davxy/jam-test-vectors/pull/90)
2025-11-24 20:12 danicuki: Thanks for clarification.
2025-11-24 20:54 celadari: Hi davxy,
Kind reminder of https://github.com/davxy/jam-test-vectors/issues/107 if you can harmonize test vectors json formats betwen `stf/preimages` and `stf/accumulate` for version 0.7.2 please :)
2025-11-24 21:17 davxy: I'll do it in https://github.com/davxy/jam-test-vectors/pull/108
  â†³ 2025-11-24 21:26 celadari: Thanks
2025-11-25 08:27 nikos-pba: GM all - Not sure if this is the best channel to ask but here I go. While structuring the JAM Course for PBA I have some questions for the teams that build on JAM:
A - Do you use specific tools that makes it easier for you to build a service? 
B - As a team - do you have an SDK (e.g.  https://hackmd.io/@polkadot/jamsdk or http://github.com/spacejamapp/jade) 
C - If you answer no on B - are you planning on having one ready? And if yes - when? [edited]
  â†³ 2025-11-25 15:46 oliver.tale-yazdi: Shilling this again https://playground.jamcha.in/ together with our C SDK https://github.com/JamBrains/service-sdk/
  â†³ 2025-11-25 15:47 oliver.tale-yazdi: Cisco used that in his presentation at the last PBA afaik
  â†³ 2025-11-25 15:47 nikos-pba: This is already noted dear sir ðŸ˜‰ 
  â†³ 2025-11-25 15:47 nikos-pba: But thank you anyways Oliver Tale-Yazdi ðŸ™
2025-11-25 15:42 hanayukii: image.png
2025-11-25 15:42 hanayukii: Hi, we would like to double-check the expected behavior of the VRF seal validation(Hs validation) in the current test vectors.

During our implementation and debugging across many combinations, we discovered:
When validating VRF seal using the postState, all test vectors pass.
But when validating using the priorState, at least two test vectors fail during header seal verification.

After our internal discussion, based on the attach diagrams and the slot-transition flow:

From the logic side we assume the VRF seal for the importing block should be validated look more likely to built from priorState?

Because the importing block header exists before entering the STF, and the postState is only produced after executing the STF, so postState should not influence validation of the block being imported.

However, in the current test vectors:
Using postState â†’ all pass
Using priorState â†’ at least two vectors fail
1756832925/00000024.json (post:ticket, prior:bandersnatch)
1756814312/00000025.json Expected H_s error can only reproduced on postState

So we would like to confirm:
Is the expected behavior of the test vectors to validate using postState?
If postState is indeed expected, we would like to understand:
Based on the diagram Why VRF validation of an importing block depends on postState?
  â†³ 2025-11-25 16:02 oliver.tale-yazdi: The current header and state do not know if the next slot author will produce a block.  
So only the kappa_prime state (check definition of H_a) can tell you who can seal [edited]
2025-11-25 16:02 dave: This is covered in section 6 of the GP, would suggest you check your implementation against that
2025-11-25 16:03 davxy: Some of the post-state bits  must be used (e.g., the validator set). The GP expressions should be aligned with this
2025-11-25 17:20 davxy: FYI, I removed the reports table from the README because it was too cumbersome to maintain. The result summaries are now published [here](https://github.com/davxy/jam-conformance/tree/main/fuzz-reports/0.7.1/summaries)

2025-11-26 10:11 danicuki: GM jammers. Could anyone help us providing a PVM trace for cases `1763488212` (block 189) and `1763371998` (block 1108)
  â†³ 2025-11-26 22:12 tomusdrw: I just rolled out an update at http://state.fluffylabs.dev/ where you can load the STF file and see our execution log with host call traces. Let me know if you need something more low-level.
  â†³ 2025-11-27 10:12 arjanz: Regarding `1763488212`, you may want to verify your constants. We spent 2 days before discovering we had a different `W_C` resulting in a storage item diff (constant was written in storage key `9fd75...ecbf2`)
  â†³ 2025-11-27 10:23 arjanz: Here are our logs when running these traces on our fuzzer target in verbose mode: https://gist.github.com/arjanz/a0202e7a329a01aff2bb33ba4ec18783
  â†³ 2025-11-27 20:47 danicuki: > <@arjanz:matrix.org> Regarding `1763488212`, you may want to verify your constants. We spent 2 days before discovering we had a different `W_C` resulting in a storage item diff (constant was written in storage key `9fd75...ecbf2`)

Indeed I discovered this morning that the issue was exactly the Constants. Thanks for the help. 
  â†³ 2025-12-05 19:00 boymaas: Was this W_C different from the 4M value?
2025-11-26 13:44 hanayukii: 
Sorry I didn't described the question clear, our cases all passed by using the state after safrole stf, but we think it looks a bit conflict with the attatched diagram(prior(11) + import_block(12) -> post(12)), so we just want to clarify it this is expected to validate after the stf.
What, we would like to clarify one specific question regarding how the single JSON test vectors are generated.

For each JSON test case, the file contains:
preState
postState
header (including the VRF seal Hs)

Our question is:
 Is the VRF seal (Hs) inside the header generated using this json test caseâ€™s postState, or using the previous blockâ€™s postState (i.e., the preState of the current JSON)?  [edited]
2025-11-26 14:44 dave: This is all specified in the GP, sections 5&6. preState = sigma, postState = sigma', header = H. IDK where the diagram is from but it is presumably only intended to give a high-level overview. The formal definition of the STF is in the GP. If that is insufficient then we can fix it, but I believe it covers your question. Have you read sections 5&6 of the GP?
2025-11-26 22:40 sourabhniyogi: For anyone who needs 0.7.1 PVM Traces for either jam-test-vectors or jam-conformance, we posted ours here:
 https://github.com/jam-duna/jamtestnet/
We use the same format as JavaJAM for PVM Traces.  

It would be great to have an online-meetup -- at the very least to see each others faces -- may I suggest Dec 9 or 16 at 5pm Lisbon time?
2025-11-27 14:29 danicuki: Thanks @sourab. This is really helput. Great idea to have a meetup. Dec 9 would be great
2025-11-27 14:32 danicuki: Could someone share the hex string of the fetch binary (c)?   I believe there is a difference in our implementation constants, which is causing mismatch with conformance tests. This is our c value:
```
0x0a00000000000000010000000000000064000000000000000200200000000c000000809698000000000080f0fa020000000000ca9a3b00000000002d3101000000000800100008000300180000000300080006005000040080000500060000fa00008070d20000093d0004000000000c00000204000000c0000080000000000c00000a000000
```

2025-11-27 14:41 danicuki: A good idea would be to have this binary documented in the conformance tests README file, it is an easy easy to check if a node is running with the same config as expected by the conformance tests
2025-11-27 15:38 dave: Hmm ideally the protocol parameters would be included in the tests themselves. In any case it is possible to get the tiny params out of `polkajam` with `polkajam --chain=dev dump-spec spec.json`. That will write out the spec for the built-in "dev" chain (which uses the tiny params) to `spec.json` as per JIP-4 (https://github.com/polkadot-fellows/JIPs/blob/main/JIP-4.md). The `protocol_parameters` field has the same format as the binary returned by the PVM fetch host-call.
2025-11-27 17:03 danicuki: Indeed we were using 0.7.2 value for W\_B - thanks for the help [edited]
2025-11-28 14:16 davxy: Hey. Please try our 0.7.2 test vectors, including both STF and traces.

https://github.com/davxy/jam-test-vectors/pull/108

2025-11-28 15:04 dakkk: > <@davxy:matrix.org> Hey. Please try our 0.7.2 test vectors, including both STF and traces.
> 
> https://github.com/davxy/jam-test-vectors/pull/108
> 

I can't see updated traces in this PR
2025-11-28 15:23 jaymansfield: Seeing the same. Only stf/accumulate and stf/reports folders look to be updated.
2025-11-28 15:39 davxy: Oops, just pushed now.
2025-11-28 20:29 davxy: FYI: upcoming PolkaJam nighty release implements GP v0.7.2.

Breaking change for Safrole tickets distribution (CE 131/132): our previous impl was incorrectly including the current epoch index (i.e., target epoch index - 1). We are now aligned with CE 131/132, and the epoch index is set to the epoch in which the ticket will actually be used (the target epoch index).
2025-11-29 12:15 davxy: Oh, I forgot to mention another Out-of-GP breaking change: we are now compliant with the [JIP-1](https://github.com/polkadot-fellows/JIPs/blob/main/JIP-1.md#host-call-specification) log specification (return `WHAT` and consume 10 gas units).
2025-11-29 12:33 davxy: These breaking changes will be explicitly included in the polkajam 0.7.2 release notes, which will be published once we receive some feedback on the test vectors rc
2025-11-30 15:45 celadari: Hi sourabhniyogi we wanted to ask you something about the stack trace of this test vector https://github.com/jam-duna/jamtestnet/blob/main/jam-test-vectors/0.7.1/fuzzy_light/00000011.log.gz

Is this stack trace only for invocation of service index = 0 ? In your running code do you have an invocation of serviceIndex = 1809622522 as well ? [edited]
2025-12-01 15:13 dakkk: Jampy is passing the new vectors (I also uploaded a 0.7.2 version of the target)
2025-12-01 16:31 danicuki: We noticed that log host call is charging 10. Previously, as far as I remember, it shouldn't charge any gas. 
2025-12-01 16:31 dave: ^^^
2025-12-01 16:37 danicuki: Ok, thanks. I missed that one. [edited]
2025-12-01 16:39 danicuki: Now jamixir is passing all test vectors as well as traces
2025-12-02 00:07 sourabhniyogi: Can we get an 0.7.1 => 0.7.2 update of https://paritytech.github.io/jam-conformance-dashboard/ (with fuzzy + fuzzy-light and maybe preimages and preimage-light) in the next week and do some performance competitions again, with a view to getting all our Baseline Audit Time (days) down again?  

I understand some of us went a little overboard  in Aug/Sept ðŸ˜… optimizing ... but I think it was a valuable lesson and worth repeating before the handoff over to this new w3f team ðŸ˜¢... who are they? [edited]
2025-12-03 17:53 sourabhniyogi: Jan Bujak For your [new gas model](https://github.com/koute/new-gas-cost-model/tree/master) can we get your [write up](https://github.com/koute/new-gas-cost-model/blob/master/graypaper.pdf) into 0.7.3 with a PR?   It would be great to wrap up the 0.7.x series this month/year with this biggie =) [edited]
2025-12-03 18:04 jan: Once the new gas cost model is merged into the GP that will become the 0.8.x.

Nevertheless, I _am_ working on getting it in the GP, however it's not that trivial as I still need to finish implementing and speccing a few extra changes (add a memory limit, add a formula to calculate memory access gas costs based on this limit, actually calculate the coefficients for that formula, switch the gas to per-block metering, remove the `sbrk` instruction and add a hostcall replacement, etc.). I'd also like it to be immediately usable, that is: once it lands in the GP we should have the test vectors, the polkajam binary, and the JAM prize fuzzer (for the W3F folks) immediately available.

There is no real point in getting it in the GP if it's incomplete and/or it's not actually fully usable yet.

The major and most important part of the model is already in my draft, so there's nothing stopping you (or anyone else) from implementing it. The algorithm itself is final and will not change in the immediate future. [edited]
2025-12-03 18:07 jan: I could rush it, but ultimately that won't change anything because the amount of work will not change, and half-assing it will just create *more* work for everyone involved.
2025-12-04 00:47 decentration: . [edited]
2025-12-05 10:40 dakkk: If there's no real point in open-sourcing the tex right now, what is the point of keeping it private? not a criticism, just a curiosity
2025-12-05 12:44 jan: I'm not keeping it private. The PDF is uploaded hence the changes are fully public. I haven't uploaded the tex just because I didn't see a point. If you want me to upload it for some inexplicable reason (you like the challenge of reading nigh inscrutable TeX equations instead of nicely typeset math?) then just say so.
2025-12-05 12:55 bamzedev: This [trace](https://github.com/davxy/jam-conformance/blob/main/fuzz-reports/0.7.1/traces/1763487981/00000050.bin), same as this [trace](https://github.com/davxy/jam-conformance/blob/main/fuzz-reports/0.7.1/traces/1763488328/00000050.bin) seems to be broken (empty pre and post state), can someone else confirm?
  â†³ 2025-12-05 13:22 oliver.tale-yazdi: I was pinging Davide yesterday with the exact same question. Both `50.bin` files do not decode for us and also do not have corresponding `.json` files, which is odd.   
I guess they are either broken on purpose or leftovers
  â†³ 2025-12-05 13:35 dakkk: > <@bamzedev:matrix.org> This [trace](https://github.com/davxy/jam-conformance/blob/main/fuzz-reports/0.7.1/traces/1763487981/00000050.bin), same as this [trace](https://github.com/davxy/jam-conformance/blob/main/fuzz-reports/0.7.1/traces/1763488328/00000050.bin) seems to be broken (empty pre and post state), can someone else confirm?

Same
  â†³ 2025-12-05 13:59 jaymansfield: Try decoding just the prestate + block from the bin file. From what I can remember these were mutated (invalid) blocks.
  â†³ 2025-12-05 15:30 davxy: I'll have a look
2025-12-05 17:51 sourabhniyogi: Jan Bujak:  We attempted to refine with `jamt` (doom WPs, we think...) in a 5+1 polkajam+us 0.7.2 testnet this week and found the following:
1. `machine` 
2. `invoke`
3. **NO** `page` host call to set up any pages memory
so, our client hit a memory violation error.  Based on our understanding, it is an absolute requirement to a [`pages` host call to setup page-level access](https://graypaper.fluffylabs.dev/#/ab2cdbd/344d03344d03?v=0.7.2) after  `machine` because of [machine having inaccessibility as the default](https://graypaper.fluffylabs.dev/#/ab2cdbd/34a50034a500?v=0.7.2) -- yes?
2025-12-05 17:54 jan: Yes, by default inner VM has its whole address space inaccessible. So if not populated beforehand it will generate a page fault that the toplevel VM needs to handle and resume execution.
2025-12-05 17:55 sourabhniyogi: ok got it - this means 0.7.2 polkajam has a bug then?  
2025-12-05 17:55 jan: I don't know. Either polkajam has a bug, or your client has a bug.
2025-12-05 17:56 dave: I don't think CoreVM maps any pages up front, it just maps them in response to page faults
2025-12-05 17:56 jan: But, what do you mean by "memory violation error"?
2025-12-05 17:56 dave: Why do you think there is a polkajam bug?
2025-12-05 17:57 jan: Assuming the toplevel VM doesn't set up any pages then resuming the inner VM would just trigger a segfault in the inner VM again.
2025-12-05 17:58 sourabhniyogi: We hit a page fault and you have a "maps them in response to page faults" type approach intentionally.  Got it.  Will look for the page fault => `pages` setup [edited]
2025-12-05 17:58 jan: Did you not implement a page faulting mechanism for inner VMs?
2025-12-05 18:00 sourabhniyogi: We did for recompiler not for interpreter -- thanks for explaining
2025-12-05 18:00 jan: Well, obviously any time your interpreter behaves differently than the recompiler you will have a bug.
2025-12-05 18:01 jan: I'd suggest writing a fuzz test or something to make sure they're in sync.
2025-12-05 18:02 jan: Or implement a single-stepping mechanism for both, and then run a progrem simultaneously in both and at every step compare the state. (That's what I do in PolkaVM.)
2025-12-05 18:02 jan: Makes it very easy to debug recompiler bugs.
2025-12-05 18:17 sourabhniyogi: Will follow your single-stepping advice, so we can get out of the log comparison business, which gets insane to do very quickly.  Great tip!  Do you think refine fuzzer protocol should incorporate this "compare every single-step" approach?  

Having seen so many tiny memory issues, I think it could help, though maybe at the basic block level instead of the instruction level?? [edited]
2025-12-05 18:28 jan: Fuzzer - no. For conformance fuzzying it's totally redundant since the final state will be different anyway in the vast majority of cases, and it would just massively slow down the testing.
2025-12-05 18:30 jan: If you'd ask me this is only really useful on instruction level, because then you will know exactly where the issue is. If you do it on a basic block level then you still have to spend the effort to analyze.
2025-12-05 18:30 dave: Can I ask what the issues you're having with log comparison are? I would think this is the way to go, with a standard log format, as then we can compare logs between implementations
2025-12-05 18:31 jan: Yes, as Dave says, in general you need to consider what you want to use it for.
2025-12-05 18:31 dave: For the fuzzer I assume you get a seed or something out on failure and can then rerun with all the logs etc enabled
2025-12-05 18:33 jan: Note that you need some form of single stepping to even be able to print out logs to know e.g. that your implementation of instruction X is wrong, otherwise you can only essentially debug when your execution would have been interrupted naturally.
2025-12-05 18:33 dave: We could of course define a standard interface for single-stepping, this just seems more complicated to me than defining a standard log format
2025-12-05 18:34 sourabhniyogi: If you have a difference at gas 100MM (or 1B), and each line in a log is like 100 bytes, you have 10GB (or 100GB) of log data, times 2 for 2 backends.  We're not going to trade files that big between ourselves, even if its compressed.
2025-12-05 18:35 dave: With single-stepping in sync you have to run the two impls on the same machine anyway?
2025-12-05 18:36 dave: Of course 100GB of log data is pretty unwieldy but it can also be useful in its own right
2025-12-05 18:41 jan: So in case this wasn't clear, my point was essentially only this: single stepping is useful if you need to debug your recompiler and/or make sure its behavior matches your interpreter. That's it. In the interpreter you can just add logging calls before each instruction (be it text or binary logs) and compare that with another implementation, no problem. But you *can't* really do this easily in the recompiler. Sure, you could codegen some code that would generate those prints, but at this point it's just a better idea to implement a generic mechanism of single stepping, and then not only you can add logs, but you can also run your recompiler step-by-step with your interpreter and "automatically" find the exact place where they disagree, without having to compare any logs.

That's it.
2025-12-05 18:45 sourabhniyogi: If we can get a pile of jamt pvm logs for multiple WP that polkajam+us agree on (in generating the same availability spec/work report hash), we'll post it in a bucket somewhere.  Thanks!
2025-12-05 19:31 boymaas: Conformance traces v0.7.1: Do they use polkajam --chain=dev params exactly? (PolkaJam v0.1.26)
  Found L mismatch which I used in v0.7.0 tests (24 vs 14,400). Are the exact constants used to run the jam-conformance traces documented somewhere? For v0.7.1 and v0.7.2?
2025-12-05 19:34 dave: IDK, would have to investigate. Possibly davxy or Alexandre Pinto can answer...
2025-12-06 07:29 clearloop: the reports say we can pass `1763489605` however in local env we can't, I can see the latest block should be failed (slot is the  same as the last success block), while the test expects state modifications, if this trace is legit? [edited]
2025-12-06 12:36 dakkk: I'd like to print it with a white background; I also would like to reference tex symbols directly on my code, like I did with other part of the GP. 
2025-12-06 17:33 sourabhniyogi: What is the responsibility of a _new_ validator (who has just been added via `designate`) to get old shards and old segments via [CE 137](https://github.com/zdave-parity/jam-np/blob/main/simple.md#ce-137-shard-distribution), [CE138](https://github.com/zdave-parity/jam-np/blob/main/simple.md#ce-138-audit-shard-request), [CE139/140](https://github.com/zdave-parity/jam-np/blob/main/simple.md#ce-139140-segment-shard-request), ... -- if the answer is "nothing", doesn't this imply an expectation that old validators who assured JAM DA for the last 28 days have a long 28-day unbonding period? 

The [shard assignment](https://github.com/zdave-parity/jam-np/blob/main/simple.md#shard-assignment) scheme appears to only apply to the immediate shards required to be held by current validator sets, with a new validator not assuming any responsibility of older validators at all? [edited]
2025-12-06 20:43 danicuki: > <@sourabhniyogi:matrix.org> What is the responsibility of a _new_ validator (who has just been added via `designate`) to get old shards and old segments via [CE 137](https://github.com/zdave-parity/jam-np/blob/main/simple.md#ce-137-shard-distribution), [CE138](https://github.com/zdave-parity/jam-np/blob/main/simple.md#ce-138-audit-shard-request), [CE139/140](https://github.com/zdave-parity/jam-np/blob/main/simple.md#ce-139140-segment-shard-request), ... -- if the answer is "nothing", doesn't this imply an expectation that old validators who assured JAM DA for the last 28 days have a long 28-day unbonding period? 
> 
> The [shard assignment](https://github.com/zdave-parity/jam-np/blob/main/simple.md#shard-assignment) scheme appears to only apply to the immediate shards required to be held by current validator sets, with a new validator not assuming any responsibility of older validators at all?

Good question. 
2025-12-06 20:46 dave: > <@sourabhniyogi:matrix.org> What is the responsibility of a _new_ validator (who has just been added via `designate`) to get old shards and old segments via [CE 137](https://github.com/zdave-parity/jam-np/blob/main/simple.md#ce-137-shard-distribution), [CE138](https://github.com/zdave-parity/jam-np/blob/main/simple.md#ce-138-audit-shard-request), [CE139/140](https://github.com/zdave-parity/jam-np/blob/main/simple.md#ce-139140-segment-shard-request), ... -- if the answer is "nothing", doesn't this imply an expectation that old validators who assured JAM DA for the last 28 days have a long 28-day unbonding period? 
> 
> The [shard assignment](https://github.com/zdave-parity/jam-np/blob/main/simple.md#shard-assignment) scheme appears to only apply to the immediate shards required to be held by current validator sets, with a new validator not assuming any responsibility of older validators at all?

New validators don't request old shards. This has been discussed before, you can probably find the discussion in the Matrix logs if you can figure out the right search terms ðŸ˜…
2025-12-06 20:48 dave: Idk about the unbonding period, not sure if there is even a way to punish validators which don't keep their segment shards
2025-12-06 20:49 dave: Maybe something will be added at some point but I don't think it's an easy problem to solve
2025-12-06 20:51 sourabhniyogi: Yipes, I was both afraid of and expecting this -- the lack of staking / unbonding / slashing is a serious problem, but its true to "bare metal" expectations:  It means service builder will have to take on this task themselves, securing JAM DA chunks because .. JAM does not, by itself, do so.  Its "Someone else's \[off topic\] problem" to reward JAM Storage providers apparently!  =) [edited]
2025-12-07 17:25 shimonchick: Maybe there is a better way, but wouldn't it be convenient for the FETCH host call to also encode the jam version used in the constants? (so we can further reason about which encoding to use in for example the INFO host call) ? [edited]
2025-12-07 17:30 dave: Yeah sounds reasonable to me, this is something to suggest in the GP channel
  â†³ 2025-12-07 17:37 shimonchick: Could you please point me to it, i believe im not there yet
2025-12-07 17:37 shimonchick: 
2025-12-07 17:37 dave: #graypaper:polkadot.io
2025-12-08 02:48 shimonchick: I am trying to run accumulation on block 2 from preimage-light traces, and getting the following error from log host call. Any suggestion what it means is appreciated:
```
INFO [boot] Bootstrap Service Accumulate, 0h @2 $18446744073709551399
WARN Panic handler called!
WARN Panic message: panicked at crates/jam-pvm-common/src/host_calls.rs:202:46:\x0ahost call returns correct type; qed: Error
```
2025-12-08 05:37 rustybot: > <@shimonchick:matrix.org> I am trying to run accumulation on block 2 from preimage-light traces, and getting the following error from log host call. Any suggestion what it means is appreciated:
> ```
> INFO [boot] Bootstrap Service Accumulate, 0h @2 $18446744073709551399
> WARN Panic handler called!
> WARN Panic message: panicked at crates/jam-pvm-common/src/host_calls.rs:202:46:\x0ahost call returns correct type; qed: Error
> ```

This occurs in the sdk support functions for `fetch` hostcall (not `log`) specifically when it attempts to decode the data returned by your hostcall
2025-12-08 21:06 sourabhniyogi: **JAM Implementers Dec Call**
Tuesday, 9 December Â· 9am PT / 5pm Lisbon / 6pm Berlin
Google Meet joining info Video call link: https://meet.google.com/sek-nctz-kry

_Agenda:_ (please add your topics!) https://hackmd.io/@sourabhniyogi/jamimplementers


2025-12-09 10:21 davxy: In case anyone missed it: https://github.com/davxy/jam-conformance/pull/112  
I plan to merge these vectors, so their visibility will decrease.

2025-12-09 16:55 sourabhniyogi: 
2025-12-09 21:21 sourabhniyogi: Thank you for the 10-11 teams who joined in today!  Lets do it again on Jan 6 or 13. 
  
Our team will post doom PVM traces matching whatever JIP spec anyone posts (binary first, with script to map polkajam into binary for memory mutation) along with some "easy" availability spec derivations, also with whatever work report derivation format anyone comes up with.   
2025-12-10 11:12 tomusdrw: Following up on the discussions about PVM logging, I drafted some proposal: https://github.com/tomusdrw/JIPs/pull/1 curious about your thoughts around that. Feel free to comment directly in the PR (it's on my fork), not fully formed proposal to w3f repo yet.
2025-12-10 19:01 danicuki: As promises, here is the short (straight to the point) tutorial to run DOOM locally (with polkajam binaries). 

https://www.youtube.com/watch?v=riyYJo-CKWE
2025-12-10 19:43 sourabhniyogi: Interested in JAM In-person Meetup in March (dates TBD but pauline-pba  is point person; location expected to be Lisbon)

Respond here as your plans develop: https://github.com/davxy/jam-conformance/discussions/93 [edited]
2025-12-11 10:31 nikos-pba: sourabhniyogi: Since yesterday I am no longer in the PBA team. As a result - I am no longer the contact person. Please contact pauline-pba about any information concerning JAM course 
2025-12-11 17:53 sourabhniyogi: On this weeks call, danicuki | Jamixir brought up Erasure Coding, which is necessary to generate work reports.  AFAICT, nothing on erasure coding has changed.  The erasure coding test vectors are here:

https://github.com/w3f/jamtestvectors/tree/master/erasure

and you may find this helpful
https://github.com/javajamio/javajam-trace/tree/main/erasure\_coding
https://github.com/jam-duna/jamtestnet/tree/main/erasurecoding
If you are concerned about performance, see [GP Issue #97](https://github.com/gavofyork/graypaper/issues/97) which I take to mean something will change next year. [edited]
2025-12-11 17:57 danicuki: Nice, please take a look https://github.com/jam-duna/jamtestnet/issues/139 - I could not get same results as you, even if our EC implementation match the test vectors
2025-12-11 17:59 sourabhniyogi: I will nominate you to generate a JIP for work report derivation =).  We will try something like Jan Bujak (ooo, be back next year) suggestion [here](https://github.com/tomusdrw/JIPs/pull/1#issuecomment-3637030827) to see how it feels. 

2025-12-11 19:33 danicuki: Do you believe this is a JIP? Or it is more a test vector specification? I am ok with what you already proposed here (maybe with one or other addition): https://github.com/jam-duna/jamtestnet/blob/0.6.4.0/guarantees/0x09cf25b4b22e936f6108cc7e74a6b31e10ea50de0e2041543d03cfdebd484dbf.json#L168-L173
2025-12-11 20:17 sourabhniyogi: No.  After thinking about Jan's intuition and the nested structure of refine + accumulate (and how much excess time we have spent this year on decoding/encoding adding/removing/moving some  attribute, reordered, etc.) its smarter to dump everything into directories with flat highly compressible files, which can cover PVM traces and ordered accumulation and work report derivations.  This  disturbs natural engineering sensibilities, but in the end, what matters is reducing debugging time, not pleasing some "did you output the log according to spec", since the logs are throwaway and only serve the goal of debugging.
2025-12-11 21:24 boymaas: That makes sense. I assume there will also be separate files for memory writes and reads. This will save enormous amount of time running through traces. 
2025-12-11 22:05 sourabhniyogi: News: https://forum.polkadot.network/t/ensuring-jam-development-sustainability-is-it-time-for-a-dedicated-jam-bounty-program/16229/8  
Follow: [Radha](https://x.com/DrW3RK)
2025-12-12 03:13 ascriv: @Radha welcome, and thanks for taking on the M1 submission evaluation.

When thereâ€™s clarity on the timeline for M1 prize distribution (subject to submissions being deemed valid), it would be helpful if you could share an update here. [edited]
2025-12-12 14:13 sourabhniyogi: Is the fellowship ratifying 0.7 GP as per [this](https://github.com/w3f/jam-milestone-delivery/pull/16/changes#diff-6b12df05e78e85aae430081274429098a4965e123ab0ea8701ada94a0141047cR71)?
2025-12-12 19:03 davxy: FWIW Tlthe polkajam-based fuzzer workflow operates correctly with GP version 0.7.1 or later.
2025-12-12 22:11 sourabhniyogi: Here is a draft on [PVM "Modular" tracing](https://github.com/jam-duna/jamtestnet/blob/main/TRACING.md) -- we got our work report difference on a doom jamt package down to just one byte (the very first leaked byte, which we believe is a child  reporting back to the parent, 7b vs 80 meaning 5 gas) -- if anyone else is working on this "Doom" WP Refine problem,  we would appreciate others posting "modular" PVM Traces so can figure out this last byte difference.  [doom.zip](https://cdn.jamduna.org/doom.zip) [edited]
  â†³ 2025-12-15 18:25 jaymansfield: I've also been working on trying to get doom to run on JavaJAM for the past week. I am now seeing the same gas difference of 5 in my calculated work reports when comparing to polkajam. 122365005 vs 122365010. 
2025-12-13 00:47 tomusdrw: Frankly I don't really understand the idea of sharing full PVM logs. Since you have all of the parts put together already (I'm quite sure you are the only team to do that so far), why don't you just swap your VM with PolkaVM and print whatever instruction-traces you want from there? Just simply by swapping your VM to PolkaVM you will already know if this is VM-level issue or some higher-level problem. In the latter case I think figuring out what that could be will be super hard by analyzing single opcodes and registers (probably without debug symbols and even source code).
2025-12-13 01:17 sourabhniyogi: Having a FFIable PVM for refinement makes a lot of sense. The size of the "print whatever instruction-traces you want" is a lot bigger than you know.  Or we are lazy about parsing it, not sure! [edited]
2025-12-13 01:29 sourabhniyogi: Unless Jan gives us the advice that we should attempt a swap, I would not follow the swap advice - if it was an option it would have been a suggestion much earlier! [edited]
2025-12-13 08:03 tomusdrw: We (typeberry) are pretty far from being able to refine doom and produce such traces, however:

1. If the issue with FFIng to PolkaVM is that you are afraid of violating some JAM prize rules, I'm guilty of already doing that for PVM Debugger. We have polkavm wrapped in [this api](https://github.com/FluffyLabs/pvm-debugger/issues/81) and wasm-bindgen, but I could try to output C-api instead so you don't have to take the risk :)
2. Heck, if we all agreed on a generic PVM API (probably not necessarily a stepping one) we could swap between each of our intepreters/compilers easily, which adds to resilence (kind of like in Ethereum you can mix&match execution and consensus clients).
3. If we had IO traces I would be able to just use our PVM to reproduce each execution even though I don't have full refine orchestration done or any host calls properly implemented (accessing db, pending changes, etc).

By IO traces I mean what Jan also mentioned in the PR, so:

1. SPI blob + Args blob
2. At each ecalli: PC + Registers + Gas
3. Trace what the host call should do with all of the data, e.g.: `read_memory(a, b) -> <blob>; set_register(7, WHO); set_memory(c, <blob2>);`

Pretty much self-contained "Execution blob" that allows (re-)starting PVM at ecalli step. That's why I was trying to focus on that kind of traces in the JIP proposal. [edited]
2025-12-13 15:51 sourabhniyogi: In response to [this API suggestion](https://github.com/davxy/jam-conformance/issues/76) Jan suggested [this](https://paritytech.github.io/matrix-archiver/archive/_21wBOJlzaOULZOALhaRh_3Apolkadot.io/index.html#$PPqn__aAWf5zP2-5bzDj1jwVT4XE1FufU2yKxGTqHHk) (aka dont look but do feel free to model an API its actually a requirement) -- where I believe the schelling point can be the [PolkaVM API](https://docs.rs/polkavm/latest/polkavm/struct.RawInstance.html).  We'll output one, maybe you could do PolkaVM for everyone else (our hero!), and probably whomever is doing a recompiler  ( Jason | JavaJAM + Prasad | Chainscore Labs ) can match ?

We'll put in your 1+2+3 in "modular" form next week, make sense -- everyone here has seen first hand that our jam-conformance bugs are 80-90%+ on host call implementations in accumulate, and refine will probably be similar.  Thank you!
2025-12-16 15:33 piet: Hey there, I just posted in the Polkadot forum but I don't want to miss out on the chance of reaching all the JAM teams here directly:

Thanks to danicuki | Jamixir , who discussed current issues of JAM teams with me directly, I'm happy to share our progress towards the first payouts of the JAM Prize. While we don't offer a solution at this point in time to cover other parts of the "unfunded areas" of JAM development, that Daniel mentions in his [forum post](https://forum.polkadot.network/t/ensuring-jam-development-sustainability-is-it-time-for-a-dedicated-jam-bounty-program/16229/10), I remain optimistic that, by staying in close communication, we can work together to identify viable paths forward.

We at the Web3 Foundation are pleased to announce that we will begin processing Milestone 1 deliveries for the JAM Prize starting January 2026.

We now have access to the necessary tooling (including the fuzzer) to test client conformance. With Gavin confirming that milestones may be accepted based on conformance to graypaper v0.7.2, we are ready to begin evaluations.

Teams are invited to submit their Milestone 1 delivery via the following [repository](https://github.com/w3f/jam-milestone-delivery).

We will evaluate all clients that conform to the most recent graypaper version. Meaning graypaper v0.7.2 or later.

Further details about the evaluation process will be communicated at a later stage. This will include, among other things, the required number of block imports a client must be able to process without error in order to be eligible for Milestone 1 payment.

Weâ€™re looking forward to reviewing your submissions and taking another important step toward greater client diversity for JAM. [edited]
  â†³ 2025-12-16 18:20 prasad-kumkar: One request: given that many implementations in set C and Dâ€“based languages does not have very performant PVM (though we aim to re-implement in low-level languages in M3), we hope it could be considered to not require fuzzing a very large number of blocks (eg. 50M-100M) that it might take months to test, or even longer when accounting for iterative fixes and re-testing
2025-12-16 17:07 ascriv: Thanks for the update.

Given the impact the delay had on implementersâ€™ planning and runway, would it be possible at some point to share a brief retrospective on the main blockers (e.g. tooling, resourcing, dependencies), and whatâ€™s changed now?

Clearer interim communication around those blockers would also be very helpful for teams planning runway and staffing for future milestones. [edited]
2025-12-17 20:21 emielsebastiaan: For what it is worth, we have added a candid JAMdot Technologies retrospective to our milestone 1 delivery submission: https://github.com/w3f/jam-milestone-delivery/blob/ffa56fa241458620362ba9ac957f12ea479590a5/deliveries/jamdottech/pyjamaz-m1.md

One key takeaway, in hindsight, is that expectations were set for us (and for many other teams) that milestone 1 audit submissions would open in Summer of 2025. In practice, given the significant number of implementation disputes that emerged from Davide Galassiâ€™s conformance fuzzing efforts, none of the teams were anywhere near true milestone 1 conformance at that time. As a result, our team came to view the period from August 2025 through December 2025 as an â€œinformal milestone 1 conformance auditâ€ phase. We are confident that we would not be as far along as we are today without the scrutiny, feedback, and iteration enabled during this period.

That said, our team (like many others) has had extensive and, at times, heated internal discussions around planning, uncertainty, and a shrinking financial runway. In this context, we would appreciate greater clarity on the process for Polkadot Fellowship membership eligibility that follows the achievement of milestone 1 conformance. Based on our reading of the JAM Prize rules, and especially everything we do not know, we assume that two members of our team would be eligible for nomination at Rank II and Rank III, respectively, and that they would be expected to substantiate their eligibility for those ranks based on their concrete contributions. From a financial planning perspective, Fellowship income would materially impact my team's ability to keep the project viable and maintain continuity of work. As such, we seek clarity from a humble and pragmatic standpoint to better align our planning with the realities of that process.
2025-12-17 22:06 rustybot: > <@emielsebastiaan:matrix.org> For what it is worth, we have added a candid JAMdot Technologies retrospective to our milestone 1 delivery submission: https://github.com/w3f/jam-milestone-delivery/blob/ffa56fa241458620362ba9ac957f12ea479590a5/deliveries/jamdottech/pyjamaz-m1.md
> 
> One key takeaway, in hindsight, is that expectations were set for us (and for many other teams) that milestone 1 audit submissions would open in Summer of 2025. In practice, given the significant number of implementation disputes that emerged from Davide Galassiâ€™s conformance fuzzing efforts, none of the teams were anywhere near true milestone 1 conformance at that time. As a result, our team came to view the period from August 2025 through December 2025 as an â€œinformal milestone 1 conformance auditâ€ phase. We are confident that we would not be as far along as we are today without the scrutiny, feedback, and iteration enabled during this period.
> 
> That said, our team (like many others) has had extensive and, at times, heated internal discussions around planning, uncertainty, and a shrinking financial runway. In this context, we would appreciate greater clarity on the process for Polkadot Fellowship membership eligibility that follows the achievement of milestone 1 conformance. Based on our reading of the JAM Prize rules, and especially everything we do not know, we assume that two members of our team would be eligible for nomination at Rank II and Rank III, respectively, and that they would be expected to substantiate their eligibility for those ranks based on their concrete contributions. From a financial planning perspective, Fellowship income would materially impact my team's ability to keep the project viable and maintain continuity of work. As such, we seek clarity from a humble and pragmatic standpoint to better align our planning with the realities of that process.

For greater visibility, I suggest also posting the message in the Let's Jam channel, as I am not sure that everyone you want to reach is here.
2025-12-18 07:04 ascriv: https://github.com/w3f/jam-milestone-delivery/blob/main/deliveries/milestone-delivery-template.md?plain=1#L28

The first milestone submission requires "Gas, trie/DB, signature-verification, and availability (EC/DB) performance tests to be run on standard hardware"

Could someone please clarify what would be considered sufficient for this requirement? [edited]
2025-12-18 07:36 dakkk: I think this is not required for M1; the form is not targeted for M1 only
2025-12-19 11:25 harpreetgill: hey there 
2025-12-19 11:25 harpreetgill: am asking about the recent changes into jam test vector 
  â†³ 2025-12-19 12:06 celadari: Which version ? 0.7.2 ?
  â†³ 2025-12-20 19:33 harpreetgill: yes
2025-12-19 11:26 harpreetgill: is it in the accumulation and reports ?
2025-12-19 11:28 harpreetgill: as i see changes are made into accumulation and reports section with newer version 
2025-12-19 15:07 shimonchick: In accumulation context, can we re-create the work item needed for fetch host call selector 14 from the work report?
2025-12-19 15:36 ascriv: Those are accumulation inputs (12.15)
2025-12-20 05:28 unknowndev: I havenâ€™t been actively working for a while, so sorry for the naive question, but is the polkajam-fuzz crate private? [edited]
2025-12-20 12:16 piet: correct, the fuzzer maintained by Parity is closed source. Only certain traces, test vectors and results are published [here](https://github.com/davxy/jam-conformance) 
2025-12-20 13:40 davxy: If you are using a ZIP-215-compliant library that is not listed [here](https://github.com/davxy/jam-conformance/pull/112), please report it in the PR.
2025-12-21 09:44 prematurata: Can someone please compute the ring commitment of
`[ "ff71c6c03ff88adb5ed52c9681de1629a54e702fc14729f6b50d2f0a76f185b3", "dee6d555b82024f1ccf8a1e37e60fa60fd40b1958c4bb3006af78647950e1b91", "7263d615d16adbe8f6888fa594f54941db9d2d7294be29d9c5f5dd69acc11ee0", "9326edb21e5541717fde24ec085000b28709847b8aab1ac51f84e94b37ca1b66", "0746846d17469fb2f95ef365efcab9f4e22fa1feb53111c995376be8019981cc", "151e5c8fe2b9d8a606966a79edd2f9e5db47e83947ce368ccba53bf6ba20a40b", ]`

Apparently these are the posterior gamma p bander keys but i have a different computed posterior gamma z. For the curious these were taken from the `christmas-batch` - `1766243113` - block `58` [edited]
2025-12-21 10:45 davxy: [Christmas batch](https://github.com/davxy/jam-conformance/pull/134)
2025-12-21 10:49 davxy: https://github.com/davxy/jam-conformance/tree/christmas-batch/fuzz-reports/0.7.2/summaries
2025-12-21 10:51 philoniare: Appreciate it, this is very helpful â€” best Christmas present of the year!
2025-12-21 15:30 vinsystems: 
2025-12-21 15:51 basedafdev: Are we allowed to use a monitoring tool like Sentry to capture and store error stack traces from fuzzer conformance test failures?
2025-12-22 07:38 harpreetgill: Hey everyone 
can we use in-build pvm , provided by jam polkadot 
to build the M1 and M2 

2025-12-22 11:00 john985: Yes, you can use it.
2025-12-22 11:39 danicuki: > <@harpreetgill:matrix.org> Hey everyone 
> can we use in-build pvm , provided by jam polkadot 
> to build the M1 and M2 
> 

Yes you can, but there are specific rules for the JAM Prize: https://jam.web3.foundation/

2
NON-PVM VALIDATING NODE PATH
2025-12-22 11:56 dakkk: But it says "For the purpose of performance testing", so I assume that you can use an existing PVM only for m3 and m4 (and my assumption should be correct since m1 and m2 prices are 100k, while m3 and m4 are 50k).

Otherwise why implement a PVM since the price for m1 and m2 is the same?
2025-12-22 11:57 dakkk: this "path" is for those that arrive at m3, and discover that their language is unable to provide the required performance
2025-12-22 12:32 ascriv: > <@dakkk:matrix.org> this "path" is for those that arrive at m3, and discover that their language is unable to provide the required performance

in that case canâ€™t you also rewrite the pvm in rust since the pvm language can be whatever you want?
2025-12-22 12:35 prematurata: I believe it was already clarified in the past that you can do that as long as it is your implementation [edited]
2025-12-22 13:24 harpreetgill: Hi everyone,
I have a question regarding the in-built PVM provided by JAM/Polkadot.

Can we use this in-built PVM to build M1 and M2?
If yes, does that mean such a project would only be eligible for submission under a non-PVM track, since we are not implementing a custom PVM ourselves?

I had asked a similar question earlier, but I wanted to clarify this distinction more precisely.
  â†³ 2025-12-22 13:34 dave: Please don't post the same message to multiple JAM channels, just post to the most relevant one
2025-12-23 13:18 muddlebee: image.png
2025-12-23 13:19 muddlebee: the extrinsic ordering in test vectors is different than what's mentioned in gray paper?

https://github.com/davxy/jam-test-vectors/blob/master/codec/tiny/extrinsic.json


  â†³ 2025-12-23 13:20 muddlebee: which one to follow as source of truth? the test vectors?
I'm new :) pls dont mind me asking dumb questions..

sorry, I just realized I asked in the wrong room ðŸ¤¦â€â™‚ï¸ [edited]
  â†³ 2025-12-23 14:17 ascriv: source of truth is always the gray paper, FYI
  â†³ 2025-12-23 14:34 rustybot: > <@muddlebee:matrix.org> which one to follow as source of truth? the test vectors?
> 
> I'm new :) pls dont mind me asking dumb questions..

Refer to expression C.16
  â†³ 2025-12-24 11:34 danicuki: json key/value (objects) do not imply order. Order is considered arbitrary and the test vectors donâ€™t impose any order. 
2025-12-24 03:07 sourabhniyogi: For host function [`provide`](https://graypaper.fluffylabs.dev/#/ab2cdbd/382a03382a03?v=0.7.2), is there anything to prevent one "sending" service from "spamming" another service via useless preimages?  The intention is that the sending service is a "factory" service from https://github.com/gavofyork/graypaper/issues/311 where the sender is the creator (via `new`) but if the sender is not the creator, it appears to open up an attack where a service could receive a lot of preimages from an attacking service, potentially choking `write` after enough preimages are sent.  Am I missing something?
2025-12-24 06:49 tomusdrw: The preimage needs to be requested by the service first, and only then anyone can provide it. Otherwise it fails early with HUH.
2025-12-24 13:27 sourabhniyogi: Got it (see [here](https://graypaper.fluffylabs.dev/#/ab2cdbd/380404380404?v=0.7.2)) thank you --  a `new` (from another "factory" service) or `solicit` must precede a successful `provide` -- thank you!

If a service is bricked through a bad `upgrade` (which can only be done through a `solicit`) where "bad" means "cannot be upgraded again", is there any way that bricked service can be rescued? [edited]
2025-12-24 15:18 shimonchick: After an Upgrade host call, if the codehash is set to zeros, if next block has guarantees for the previous service that used to have code, should the guarantee error out or be skipped?
2025-12-25 18:09 shimonchick: Has somebody noticed weird behavior with block 180 from the fuzzy set? Its an epoch rotation so fallback keys should be recalculated, but according to the post-state they are not. am i missing something?
2025-12-26 10:08 shimonchick: Do minifuzz self-test examples exist for 0.7.2 ? We've developed an implementation that now passes 0.7.2 test vectors, but due to the 0.7.0 codec differences our state root differs for the previous version. Would it be sensible to get an eventual update on the minifuzz examples or do we need to go back and implement things properly for the previous version as well? [edited]
2025-12-26 10:17 rustybot: > <@shimonchick:matrix.org> Do minifuzz self-test examples exist for 0.7.2 . We've developed an implementation that now passes 0.7.2 test vectors, but due to the 0.7.0 codec differences our state root differs for the previous version. Would it be sensible to get an eventual update on the minifuzz examples or do we need to go back and implement things properly for the previous version as well?

The update has already been scheduled; see https://github.com/davxy/jam-conformance/issues/111.

At this point, this is expected to be provided after the Christmas holidays.
  â†³ 2025-12-26 10:25 shimonchick: Amazing, thanks 
2025-12-26 18:49 celadari: Hi guys,

Small question, about test conformance `1766243315_1733/00000003.json`, we see this

```json
"author_index": 65535,
```

I don't think this is valid according to GP https://graypaper.fluffylabs.dev/#/ab2cdbd/0c8e010c9601?v=0.7.2

H\_A = Kappa'\[H\_I\] but we don't use the "cycle" symbol so from my understanding of the GP => author\_index should not be bigger than V ? [edited]
2025-12-27 16:07 celadari: Hi guys,

I have a doubt regarding jam-comformance test vector `1766243113` file 00000058.json    https://github.com/davxy/jam-conformance/blob/main/fuzz-reports/0.7.2/traces/1766243113/00000058.json

For the following bandersnatch keys:

```rust
let public_keys = vec![
        hex::decode("ff71c6c03ff88adb5ed52c9681de1629a54e702fc14729f6b50d2f0a76f185b3").expect("Decoding hex string failed"),
        hex::decode("dee6d555b82024f1ccf8a1e37e60fa60fd40b1958c4bb3006af78647950e1b91").expect("Decoding hex string failed"),
        hex::decode("7263d615d16adbe8f6888fa594f54941db9d2d7294be29d9c5f5dd69acc11ee0").expect("Decoding hex string failed"),
        hex::decode("9326edb21e5541717fde24ec085000b28709847b8aab1ac51f84e94b37ca1b66").expect("Decoding hex string failed"),
        hex::decode("0746846d17469fb2f95ef365efcab9f4e22fa1feb53111c995376be8019981cc").expect("Decoding hex string failed"),
        hex::decode("151e5c8fe2b9d8a606966a79edd2f9e5db47e83947ce368ccba53bf6ba20a40b").expect("Decoding hex string failed"),
    ];
```

we're supposed to get the following ring root `0xb44239ecc9ff5ad60c2791dfe1dcdafcc0e478d9eca63b24999750b724a0676deb70a059fab442fdb05b007fe22f93a585ba7b113e3fba1f1880b317a4748bb4f3446a35793ca94f8b75a6654fa259931eec4ede39e96a9da83f2c8785aa89f892e630ae2b14e758ab0960e372172203f4c9a41777dadd529971d7ab9d23ab29fe0e9c85ec450505dde7f5ac038274cf`   but we're

actually getting `0xacfbe5e61d0b6b9a32476dd94ba74a6cca3634503f9dadb9cb164fd448d471a8c3e3f57e770c304944a744cfa43e07d9a888e4565c538749b08138d0d0eb4a1447d2e3f8504ee885b792925a9df49524637cd8d43177beeacfcb348babe9d90692e630ae2b14e758ab0960e372172203f4c9a41777dadd529971d7ab9d23ab29fe0e9c85ec450505dde7f5ac038274cf`

Our ring roof construction is  based on https://github.com/davxy/bandersnatch-vrf-spec/blob/main/assets/example/src/main.rs

Did someone also run into the same issue ?
2025-12-27 16:15 dakkk: > <@celadari:matrix.org> Hi guys,
> 
> I have a doubt regarding jam-comformance test vector `1766243113` file 00000058.json    https://github.com/davxy/jam-conformance/blob/main/fuzz-reports/0.7.2/traces/1766243113/00000058.json
> 
> For the following bandersnatch keys:
> 
> ```rust
> let public_keys = vec![
>         hex::decode("ff71c6c03ff88adb5ed52c9681de1629a54e702fc14729f6b50d2f0a76f185b3").expect("Decoding hex string failed"),
>         hex::decode("dee6d555b82024f1ccf8a1e37e60fa60fd40b1958c4bb3006af78647950e1b91").expect("Decoding hex string failed"),
>         hex::decode("7263d615d16adbe8f6888fa594f54941db9d2d7294be29d9c5f5dd69acc11ee0").expect("Decoding hex string failed"),
>         hex::decode("9326edb21e5541717fde24ec085000b28709847b8aab1ac51f84e94b37ca1b66").expect("Decoding hex string failed"),
>         hex::decode("0746846d17469fb2f95ef365efcab9f4e22fa1feb53111c995376be8019981cc").expect("Decoding hex string failed"),
>         hex::decode("151e5c8fe2b9d8a606966a79edd2f9e5db47e83947ce368ccba53bf6ba20a40b").expect("Decoding hex string failed"),
>     ];
> ```
> 
> we're supposed to get the following ring root `0xb44239ecc9ff5ad60c2791dfe1dcdafcc0e478d9eca63b24999750b724a0676deb70a059fab442fdb05b007fe22f93a585ba7b113e3fba1f1880b317a4748bb4f3446a35793ca94f8b75a6654fa259931eec4ede39e96a9da83f2c8785aa89f892e630ae2b14e758ab0960e372172203f4c9a41777dadd529971d7ab9d23ab29fe0e9c85ec450505dde7f5ac038274cf`   but we're
> 
> actually getting `0xacfbe5e61d0b6b9a32476dd94ba74a6cca3634503f9dadb9cb164fd448d471a8c3e3f57e770c304944a744cfa43e07d9a888e4565c538749b08138d0d0eb4a1447d2e3f8504ee885b792925a9df49524637cd8d43177beeacfcb348babe9d90692e630ae2b14e758ab0960e372172203f4c9a41777dadd529971d7ab9d23ab29fe0e9c85ec450505dde7f5ac038274cf`
> 
> Our ring roof construction is  based on https://github.com/davxy/bandersnatch-vrf-spec/blob/main/assets/example/src/main.rs
> 
> Did someone also run into the same issue ?

We discussed this on graypaper Channel few days ago 
  â†³ 2025-12-27 16:16 celadari: if you can point me to the discussion in GP, otherwise I'll scroll up :)
  â†³ 2025-12-27 16:17 celadari: I think I found it
  â†³ 2025-12-27 16:20 celadari: Thanks dakkk | JamPy ðŸ«¶ It solved my error âœ¨ï¸
  â†³ 2025-12-27 16:21 dakkk: Great, my pleasure ðŸ‘
2026-01-01 10:11 clearloop: if the fork logic is updated? for `1766241968`, after the main chain finalized at slot 9, the set has an incoming block `4294967292` which will be rejected, but it expects the post state at slot 8, reports say we can pass this test however in my local testing environment, I have no ideas how to process sets like this [edited]
  â†³ 2026-01-01 21:21 clearloop: got if fixed by search the parent by hash, if not exists, reset the state
2026-01-04 07:04 clearloop: https://github.com/davxy/jam-conformance/discussions/145

any team tried `1766565819_4872` locally? could you please recheck davxy ? 
  â†³ 2026-01-04 11:48 yu2c: We tested spacejam fuzzer + our target, and both cases report invalid extrinsic hash for 00000040 and 00000041.
However, when using our own fuzzer + the same target, we are able to reproduce the exact extrinsic hash as recorded in the block header.
  â†³ 2026-01-06 12:21 clearloop: out fuzzer is affected since we failed to decode the JSON > \< [edited]
2026-01-04 08:11 dakkk: > <@clearloop:matrix.org> https://github.com/davxy/jam-conformance/discussions/145
> 
> any team tried `1766565819_4872` locally? could you please recheck davxy ? 

Maybe the invalid extrinsic is the expected result? 
  â†³ 2026-01-04 08:36 clearloop: nope, the post state expects the block to be accepted
  â†³ 2026-01-04 10:40 dakkk: > <@clearloop:matrix.org> nope, the post state expects the block to be accepted

I will check this afternoon on jampy ðŸ‘
  â†³ 2026-01-04 14:33 jaymansfield: 39/41 pass. I reject 40 for duplicate core indexes in guarantees.
  â†³ 2026-01-04 15:36 dakkk: I tested again on jampy, block 39 is ok, block 40 is rejected for invalid guarantee order, and block 41 is ok; the trace for block 40 has the same pre and post state root, so rejected the block is correct
  â†³ 2026-01-05 16:06 clearloop: Thanks! so likely I got some problems in our implementations, I thought the ex hash check is conflict with other tests
  â†³ 2026-01-06 11:40 clearloop: we were doing json decoding for our local tests that we failed to decode the `output_oversize` result which caused us failed to validate the signature for block `41`
 [edited]
2026-01-07 17:20 decentration: hey davxy , im having a quite a hair pulling time executing service blob in v0.7.1, spending a long while trying to conform to it, is there a dissembled version of it for code hash `0xee79d0b64091d826e5317add07c3751dbd42a536cfc4482204adf0705561a92d`, which is in stf/accumulate of the davxy/jam-test-vectors? [edited]
2026-01-08 12:16 davxy: https://github.com/davxy/jam-conformance/discussions/148
2026-01-08 18:32 shimonchick: In what order are you processing extrinsics for a single block. I noticed that execution order matters and cannot be completely parallelized (for example assurances before guarantees and guarantees before accumulation). Also disputes before accumulation. I am curious if teams have found other practical tips [edited]
2026-01-08 18:52 tomusdrw: > <@shimonchick:matrix.org> In what order are you processing extrinsics for a single block. I noticed that execution order matters and cannot be completely parallelized (for example assurances before guarantees and guarantees before accumulation). Also disputes before accumulation. I am curious if teams have found other practical tips

Did you try checking in the Gray Paper?
2026-01-08 22:17 davxy: https://github.com/davxy/jam-conformance/pull/147
2026-01-09 13:50 decentration: can someone let me know if they were able to successfully execute the service blob within the test vectors such as [process_one_immediate_report-1](https://github.com/davxy/jam-test-vectors/blob/7037a037aa08c43ae98f758ad2e737c530446a02/stf/accumulate/tiny/process\_one\_immediate\_report-1.json#L110C38-L110C71496), which contains the service blob `id: 1729` on any version particularly `0.7.2`? I am able to pass vectors such as `work_ejected_for_service-1`, but when it comes to `1729` i am getting a a deadlock.

If i put the blob into [fluffy labs debugger](https://pvm.fluffylabs.dev/?#/) and run it with typeberry/pvm, it also panicked:

```
Host Call
PVM execution reached a log host call.

Host Call:
log
(index: 100)
Use generic UI
Level:
WARNING
Message:
Panic message: panicked at crates/jam-pvm-common/src/host_calls.rs:202:46:
host call returns correct type; qed: Error
Target: 0x0 (0 bytes)
Message: 0xfefdf928 (117 bytes)
```

so this has lead me to question the vector over my sanity. Any confirmation either way from anyone would be greatly appreciated. [edited]
  â†³ 2026-01-09 14:35 tomusdrw: here you can find the execution trace:

```
INFO  [test-runner] Preparing tests for 1 files.
INFO  [test-runner] Running 1 tests.
LOG   [test-runner] Tests registered successfully
INFO  [test-runner] Running accumulate tests [1/1]
LOG   [test-runner] [accumulate:ananas] running test from ../test-vectors/w3f-davxy_072/stf/accumulate/tiny/process_one_immediate_report-1.json (spec: tiny)
TRACE [test-runner]  {
  input: { slot: 43, reports: [ [WorkReport], [length]: 1 ] },
  pre_state: {
    slot: 42,
    entropy: Bytes { raw: [Uint8Array], length: 32 },
    ready_queue: [
      [Array],      [Array],
      [Array],      [Array],
      [Array],      [Array],
      [Array],      [Array],
      [Array],      [Array],
      [Array],      [Array],
      [length]: 12
    ],
    accumulated: [
      [Array],      [Array],
      [Array],      [Array],
      [Array],      [Array],
      [Array],      [Array],
      [Array],      [Array],
      [Array],      [Array],
      [length]: 12
    ],
    privileges: {
      bless: 0,
      assign: [Array],
      designate: 0,
      register: 0,
      always_acc: [Array]
    },
    statistics: undefined,
    accounts: [ [InMemoryService], [length]: 1 ]
  },
  output: { ok: Bytes { raw: [Uint8Array], length: 32 } },
  post_state: {
    slot: 43,
    entropy: Bytes { raw: [Uint8Array], length: 32 },
    ready_queue: [
      [Array],      [Array],
      [Array],      [Array],
      [Array],      [Array],
      [Array],      [Array],
      [Array],      [Array],
      [Array],      [Array],
      [length]: 12
    ],
    accumulated: [
      [Array],      [Array],
      [Array],      [Array],
      [Array],      [Array],
      [Array],      [Array],
      [Array],      [Array],
      [Array],      [Array],
      [length]: 12
    ],
    privileges: {
      bless: 0,
      assign: [Array],
      designate: 0,
      register: 0,
      always_acc: [Array]
    },
    statistics: undefined,
    accounts: [ [InMemoryService], [length]: 1 ]
  }
}
LOG   [accumulate] Accumulating service 1729, transfers: 0 operands: 1 at slot: 43
INSANE [host-calls-pvm] [1729] [PC: 17451] Invoking Fetch:1.  Gas: 99962. Regs: .
TRACE [host-calls] [1729] FETCH(0) <- 0x0a000000000000000100000000000000...00c0000080000000000c00000a000000 (134 bytes)
INSANE [host-calls] [1729] FETCH(0) <- 0x0a00000000000000010000000000000064000000000000000200200000000c000000809698000000000080f0fa020000000000ca9a3b00000000002d3101000000000800100008000300180000000300080006005000040080000500060000fa00008070d20000093d0004000000000c00000204000000c0000080000000000c00000a000000
INSANE [host-calls-pvm] [1729] [PC: 17451] Result Fetch:1.  Gas: 99952. Regs: r07=134 (0x86).
INSANE [host-calls-pvm] [1729] [PC: 17651] Invoking Fetch:1.  Gas: 99709. Regs: r07=206752 (0x327a0), r09=134 (0x86).
TRACE [host-calls] [1729] FETCH(0) <- 0x0a000000000000000100000000000000...00c0000080000000000c00000a000000 (134 bytes)
INSANE [host-calls] [1729] FETCH(0) <- 0x0a00000000000000010000000000000064000000000000000200200000000c000000809698000000000080f0fa020000000000ca9a3b00000000002d3101000000000800100008000300180000000300080006005000040080000500060000fa00008070d20000093d0004000000000c00000204000000c0000080000000000c00000a000000
INSANE [host-calls-pvm] [1729] [PC: 17651] Result Fetch:1.  Gas: 99699. Regs: r07=134 (0x86), r09=134 (0x86).
INSANE [host-calls-pvm] [1729] [PC: 2281] Invoking LogHostCall:100.  Gas: 98232. Regs: .
TRACE [host-calls] [1729] LOG(1729, INFO(2), , This is Accumulate in the Test Service 6c1h with 1 items)
INSANE [host-calls-pvm] [1729] [PC: 2281] Result LogHostCall:100.  Gas: 98222. Regs: .
INSANE [host-calls-pvm] [1729] [PC: 17451] Invoking Fetch:1.  Gas: 98057. Regs: r10=14 (0xe).
TRACE [host-calls] [1729] FETCH(14) <- 0x01002a762f984bcb960c32e78fb31b64...d36b4ce7a69729081481c50119dbe400 (152 bytes)
INSANE [host-calls] [1729] FETCH(14) <- 0x01002a762f984bcb960c32e78fb31b648c3bab5b8ebc0ac2b945f936d30cdc4bec9c51f9df1925c1adae33dc75126996fe1200fe80fcc27e70c177fa259081296236bf52de5ab6e18f3a407ae0952c67b624a56a6eafd7c40feb2ade331a18b4c929038a8a7fea081877c200733e9aad0bdf3c1f43ea76cf5c3235d70dce8c58a84ac1a0860010efd36b4ce7a69729081481c50119dbe400
INSANE [host-calls-pvm] [1729] [PC: 17451] Result Fetch:1.  Gas: 98047. Regs: r07=152 (0x98), r10=14 (0xe).
INSANE [host-calls-pvm] [1729] [PC: 17651] Invoking Fetch:1.  Gas: 97768. Regs: r07=206752 (0x327a0), r09=152 (0x98), r10=14 (0xe).
TRACE [host-calls] [1729] FETCH(14) <- 0x01002a762f984bcb960c32e78fb31b64...d36b4ce7a69729081481c50119dbe400 (152 bytes)
INSANE [host-calls] [1729] FETCH(14) <- 0x01002a762f984bcb960c32e78fb31b648c3bab5b8ebc0ac2b945f936d30cdc4bec9c51f9df1925c1adae33dc75126996fe1200fe80fcc27e70c177fa259081296236bf52de5ab6e18f3a407ae0952c67b624a56a6eafd7c40feb2ade331a18b4c929038a8a7fea081877c200733e9aad0bdf3c1f43ea76cf5c3235d70dce8c58a84ac1a0860010efd36b4ce7a69729081481c50119dbe400
INSANE [host-calls-pvm] [1729] [PC: 17651] Result Fetch:1.  Gas: 97758. Regs: r07=152 (0x98), r09=152 (0x98), r10=14 (0xe).
INSANE [host-calls-pvm] [1729] [PC: 8301] Invoking Write:4.  Gas: 96005. Regs: r07=66232 (0x102b8), r08=4 (0x4), r09=207168 (0x32940), r10=16 (0x10).
TRACE [host-calls] [1729] WRITE(0x6c617374, 0xefd36b4ce7a69729081481c50119dbe4 (16 bytes)) <- OK: null
INSANE [host-calls-pvm] [1729] [PC: 8301] Result Write:4.  Gas: 95995. Regs: r07=18446744073709551615 (0xffffffffffffffff), r08=4 (0x4), r09=207168 (0x32940), r10=16 (0x10).
LOG   [accumulate] Accumulation successful for 1729. Consumed: 4364
```

It does not work on the PVM debugger if you just keep clicking "Run" because the test calls FETCH two times (technically 4, but requests two different data types):

1. The first request is for constants which the PVM debugger has hardcoded (so works fine)
2. The second request is for operands which we don't really have - the debugger passes the constants data again, and that fails.

You can find the correct data in the execution trace, so pasting these as output in the debugger should give you the same result. [edited]
  â†³ 2026-01-09 20:22 decentration: hey thanks for this, i see now that i can reach HALT, and use 4262 out of the expected 4364 accumulate\_gas\_used, i am now wondering where that 102 gas went. [edited]
  â†³ 2026-01-20 22:18 decentration: hey tomusdrw im testing my pvm execution vs typeberry on the pvm-debugger, with the fetch operands you provided,  and they both match in terms of pvm instruction level execution, however there seems to be some kind of protocol overhead gas that i cannot work out where it comes from, where my implementation is 140 gas under Typeberry. Any idea where this protocol overhead is from? [edited]
  â†³ 2026-01-21 10:38 tomusdrw: does your gas values after host calls match mine? gas difference might also mean that your implementation took different branches in the pvm code - which would signify that either it was fed with different data or it read that data incorrectly.
  â†³ 2026-01-21 10:38 tomusdrw: I can also share instruction-level trace with gas after each and every instruction if that would help
  â†³ 2026-01-21 10:39 tomusdrw: however you can just step over execution yourself in the debugger and compare at what point you start to diverge
  â†³ 2026-01-21 22:03 decentration: ive done a 3 way pairwise comparison of traces of mine vs ananas vs typeberry, and i believe now i have made my pvm execution conformant in both instructions and gas, the way i was handling ecalli +host call was different.
  â†³ 2026-01-21 22:03 decentration: that would help for a sanity check : )
2026-01-10 18:25 davxy: https://github.com/davxy/jam-conformance/pull/151
2026-01-11 01:41 clearloop: for the conformance repo, if it is possible to move the `tar.gz` stuffs to releases? embedding them in the repo make it hard to pull new commits

```
Error downloading object: fuzz-proto/examples/0.7.0.tar.gz (c242cfa): Smudge error: Error downloading fuzz-proto/examples/0.7.0.tar.gz (c242cfa9e8565e27f3033a35a4407d87309e1e49ef85d6ea5fd16f937cd0ed35): batch response: This repository exceeded its LFS budget. The account responsible for the budget should increase it to restore access.
``` [edited]
2026-01-11 19:24 davxy: I completely overlooked GitHub LFS quota limits; I will remove the files from LFS.


2026-01-12 13:44 danicuki: you can skip large files using:
```
GIT_LFS_SKIP_SMUDGE=1 git
```
2026-01-12 16:29 davxy: ðŸŽ  https://paritytech.github.io/jam-conformance-dashboard/ 
2026-01-13 15:26 clearloop: if there is max depth for the forks in fuzz tests? the current forks in fuzz tests are tracking header hash but not just in one-slot level, which makes it hard to truncate the block history in memory [edited]
2026-01-13 16:49 rustybot: > <@clearloop:matrix.org> if there is max depth for the forks in fuzz tests? the current forks in fuzz tests are tracking header hash but not slot, which makes it hard to truncate the block history in memory

max fork depth is 1, new blocks are never built over mutations

https://github.com/davxy/jam-conformance/blob/main/fuzz-proto/README.md#forking-m1
  â†³ 2026-01-14 10:21 clearloop: for example, `1767872928_8840`, the blocks are queued like this:

```
37(genesis)
-> 38
  -> 39 (failed)
  -> 39
  -> 47
```

block 47 is a child of block 38, I was worrying about if there will be cases like block 48 has parent 37 previously, so likely the answer is no? [edited]
  â†³ 2026-01-14 17:10 rustybot: > <@clearloop:matrix.org> for example, `1767872928_8840`, the blocks are queued like this:
> 
> ```
> 37(genesis)
> -> 38
>       -> 39 (failed)
>       -> 39
>       -> 47
> ```
> 
> block 47 is a child of block 38, I was worrying about if there will be cases like block 38 has parent 37 previously, so likely the answer is no?

The answer is no
2026-01-15 19:23 shimonchick: For trace 1766565819_2010 block 222 can we have the json as well as the .bin?
2026-01-16 13:39 shimonchick: Also when are refine/auth invocation test vectors planned? Is there a timeline/roadmap published somewhere?
2026-01-16 19:08 sourabhniyogi: I attempted to submit a Fellowship ref to ratify version 0.7.2 (based on [this PR](https://github.com/w3f/jam-milestone-delivery/pull/16/changes)) using https://collectives.subsquare.io/ with a system remark "Ratify 0.7.2 JAM https://github.com/gavofyork/graypaper/releases/tag/v0.7.2" but got a `Bad Origin` [here](https://collectives-polkadot.subscan.io/extrinsic/8025568-2) which I gather means that only a Fellowship member can submit refs?

If I'm right about this, can someone actually in the fellowship submit a proposal to ratify 0.7.2 -- thank you!

If not, let me know what I did wrong and I'll try again .. or maybe you can try yourself? [edited]
2026-01-18 11:08 shimonchick: Does polkajam currently have a way to pass in a JIP-4 chainspec file? I couldnt find proper usage instructions
  â†³ 2026-01-18 14:57 shimonchick: For future reference, use:
```
./polkajam --chain=dev dump-spec dev-spec.json
./polkajam --chain=dev-spec.json run --dev-validator 0
``` [edited]
2026-01-18 13:32 dave: Yes, use `--chain=<path-to-chainspec-file>`
2026-01-19 20:11 ascriv: I think thereâ€™s a corner case in gp which might need looking at: for branch and djump itâ€™s able to â€œlook aheadâ€ to tell if weâ€™re jumping to a basic block beginning and panic if so. But fallthrough canâ€™t, so the counter would end up in a spot which isnâ€™t a basic block beginning. Should we allow it to lookahead similarly for consistency between all terminating opcodes which arenâ€™t trap? [edited]
2026-01-19 20:39 ascriv: As an example for why this seems weird to me, if we just finished a block and the counter is now off the tape, we are supposed to spend 1 gas to run a trap if the block termination was a fallthrough, but panic immediately spending no gas if the block termination was a branch or djump   [edited]
2026-01-20 17:17 shimonchick: Currently, is there a way to test/fuzz conformance with JAMNP? Other than connecting to polkajam nodes
2026-01-22 16:10 ascriv: @[Piet | W3F] Quick check-in: is conformance testing still expected to start sometime this month, or has the timeline shifted? Just want to plan accordingly.
2026-01-22 17:16 oliver.tale-yazdi: Cant speak for him ofc, but looks like at least the fuzzing for that has started https://github.com/w3f/jam-milestone-delivery/pull/12#issuecomment-3768891445
2026-01-22 17:21 dakkk: the other question is whether they're proceeding in random order or by submission date [edited]
2026-01-22 17:26 ascriv: Is W3F going by when the PR for the submission was opened? Several teams did this before they had valid 0.7.2 implementations (trivially true because some were made in June of last year), so I think it couldn't be the canonical ordering [edited]
2026-01-22 17:26 ascriv: @[Piet | W3F] FYI^
2026-01-22 17:44 emielsebastiaan: ima_69e561c.jpeg
2026-01-22 17:44 emielsebastiaan: In addition to the questions regarding the timing and sequencing of audits, I would like to seek clarification on Gavinâ€™s remark concerning submission and resubmission requirements relative to the most recently published Gray Paper version. This question is prompted in particular by the following comment: https://github.com/w3f/jam-milestone-delivery/pull/16#issuecomment-3778564389.

Specifically, Gray Paper version 0.8 is expected to be published imminently. If W3Fâ€™s audit of a teamâ€™s M1 submission results in a dispute that necessitates changes to the implementation after the publication of version 0.8, would the revised submission be required to conform to Gray Paper 0.8 rather than the version that was current at the time of the original submission?
2026-01-22 17:57 ascriv: looks like activity is happening here https://github.com/w3f/jam-conformance FYI all. worth a "watch" on this 
2026-01-24 14:46 ascriv: boosting https://github.com/davxy/jam-conformance/discussions/160
  â†³ 2026-01-24 14:58 clearloop: likely these are the two cases that all impls "failed" on them? [edited]
2026-01-25 14:57 piet: Hey all,

We have shared initial results with a few teams and notified them about their shortcomings without clearly communicating our requirements for M1 conformance fuzzing. Iâ€™d like to quickly update all teams on this matter. We will require your implementations to complete **1 million steps** without error.  In order to accelerate the process, we might split it into 10 consecutive fuzzing runs, each with at least 100 thousand steps. Have a nice Sunday!
2026-01-25 21:12 james233: Hi team, is their any demo implementation of PVM available? [edited]
2026-01-26 05:36 jan: By "demo" you mean a public PVM implementation?

https://github.com/paritytech/polkavm

NOTE: You're not allowed to look at a big chunk of these sources if you're participating in the JAM Prize.
2026-01-27 12:06 ascriv: @[Piet | W3F] 
Thanks for the clarification on the M1 fuzzing requirements that helps a lot.

Quick process question: if a team doesnâ€™t pass the 1M-step fuzzing requirement on an initial attempt, whatâ€™s the expected policy?

Do you generally plan to iterate with teams until they reach conformance, or is there a point where you move on to other implementations?
  â†³ 2026-01-27 12:41 piet: We are most likely to address this topic end of this week in addition to updated terms and conditions. We are planning to use GitHub and the PRs as a queue for processing the submissions. Oldest PRs will be checked first. We will then most likely put an upper bound to the evaluation process  once shortcomings have been identified and shared. If the team manages to rectify in time, they can be proceed with the fellowship interview and eventually be awarded the prize. If a team doesn't manage to rectify in time they'll need to resubmit and be put at the back of the queue. This should speed up the whole process as unfinished implementations can't block the queue for a prolonged period of time.
  â†³ 2026-01-27 13:09 boymaas: Makes sense as a process. One thought: first teams face untested traces (potentially with inconsistencies) and do the work that hardens the path for everyone behind them. As traces and fixes get posted, later teams benefit from a growing body of known edge cases â€” more traces, fewer surprises. So this works well for queue throughput, but not so much for fairness toward the first teams. 
  â†³ 2026-01-27 13:14 rustybot: The alt is to keep the results secret?
  â†³ 2026-01-27 13:17 shimonchick: Isnt it net-positive for everyone to have access to the traces and to have hardened implementations sooner? Essentially we are trading off fairer evaluation (or slower processing if someone goes to the back of the queue) for slower multi-client consensus 
  â†³ 2026-01-27 13:21 boymaas: Or remove the information asymmetry: do round-based evaluation. Each period all teams get fuzzed, all traces are public, everyone gets the same feedback simultaneously. Teams fix and prepare for the next round. When they pass, submission order determines priority.
  â†³ 2026-01-27 13:39 ascriv: Maybe something like all teams start on batch 0; every day thereâ€™s a run for every team for their current batch. if a team passes, their batch index increases. Results could be public or private [edited]
2026-01-30 21:15 jaymansfield: Started a new discussion about a trace here: https://github.com/davxy/jam-conformance/discussions/164
2026-01-30 21:33 dakkk: Can you link the point where you see the post state as 0? I can't find it and traces runs ok on jampy test runner
2026-01-30 21:41 jaymansfield: 
2026-01-30 21:45 dakkk: image.png
2026-01-30 21:55 jaymansfield: Oops I had the wrong link. This one: https://github.com/w3f/jam-conformance/tree/pyjamaz_m1/fuzz-reports/0.7.2/traces/1769559762_4234
2026-01-30 22:16 sourabhniyogi: 
2026-01-30 22:18 ascriv: I would guess it's an issue with the trace itself, because traces for invalid block test vectors usually have the post state equalling the pre state (e.g. https://github.com/w3f/jam-conformance/blob/pyjamaz_m1/fuzz-reports/0.7.2/traces/1766243315_8065/00000038.json)


2026-01-30 22:21 ascriv: but the trace in question (https://github.com/w3f/jam-conformance/blob/pyjamaz_m1/fuzz-reports/0.7.2/traces/1769559762_4234/00007689.json) ends with 

 "post_state": {
        "state_root": "0x0000000000000000000000000000000000000000000000000000000000000000",
        "keyvals": []
    }
2026-01-30 22:21 ascriv: something wrong with when this trace was generated, maybe a fuzzer issue? dunno
2026-02-04 01:09 sourabhniyogi: Congratulations to TSJAM on reaching 1MM steps -  We are so happy for you! ðŸ¥°ðŸ¤—

Concerning "let me know if I should share to someone else. I just sent an invite in github for you to be acollaborator of the sourcecode." vs "the rules of the JAM Prize require your repository of the JAM client to be open source" here https://github.com/w3f/jam-milestone-delivery/pull/19#issuecomment-3841867798 ...  I had thought:

- most teams would be (and are) closed source to support general "clean room" requirements
- every team would happily invite w3f + fellowship evaluators (who are not part of a JAM team)
- every team would go "full" open source in M4/M5 timeframe in full accordance with Rule 26

I believe this notion is worth aiming for, and that the "I just sent an invite..." pattern be dominant.  More than a few teams have M2/M3 type code (and more than few do not) I'm sure, and we should continue to strive for max clean room compliance, which is best supported by the "I just sent an invite..." pattern, with most all teams continuing "closed source" for most of 2026.

Did this notion disappear? [edited]
2026-02-04 02:49 shimonchick: Can somebody let me know what is the case with block 76 in this trace? Are there any padding points/invalid keys? https://github.com/w3f/jam-conformance/tree/pyjamaz_m1/fuzz-reports/0.7.2/traces/1768066437_2547
In my implementation it fails to verify the fallback signature, while it should process correctly
2026-02-04 09:27 clearloop: Anyone from w3f can help us connect `midegdugarova`? the performance page says we can be reviewed in 2 days, while after 1 month we don't even have a branch for checking our status XD, I asked on the PR and there is no response
2026-02-04 09:30 piet: Hey Tianyi | SpaceJam , sorry for the delay on our side here. Mideg has a lot of other tasks outside of JAM. I'll check with her, but I'm confident that you will receive an update shortly. Thanks for the reminder. 
  â†³ 2026-02-04 09:31 clearloop: thanks! ðŸ™
2026-02-04 09:36 mideg: hi Tianyi | SpaceJam . the feedback will be shared shortly (by EOW)
2026-02-05 09:09 dakkk: it's a quite good concern, we spent hundreds of hours implementing and fixing our implementations [edited]
2026-02-07 15:08 jaymansfield: Are there any estimates on when the M2 fuzzer will be ready?
